Ä°ÅŸte Ã¼Ã§Ã¼ncÃ¼ metnin TÃ¼rkÃ§eye Ã§evrilmiÅŸ hali, konu baÅŸlÄ±klarÄ± ve emojilerle birlikte:

---

### ğŸ§  BÃ¼yÃ¼k Dil Modellerine Genel BakÄ±ÅŸ

[MÃœZÄ°K] BÃ¼yÃ¼k dil modelleri her yerdeler. BazÄ± ÅŸeyleri inanÄ±lmaz derecede doÄŸru yapÄ±yorlar, bazÄ± ÅŸeyleri ise oldukÃ§a ilginÃ§ ÅŸekilde yanlÄ±ÅŸ. Ben Marina Danilevsky, IBM Researchâ€™te KÄ±demli AraÅŸtÄ±rma Bilimcisiyim ve size bÃ¼yÃ¼k dil modellerinin daha doÄŸru ve gÃ¼ncel olmasÄ±na yardÄ±mcÄ± olacak bir Ã§erÃ§eveden bahsetmek istiyorum: Retrieval-Augmented Generation (RAG), yani "geri getirme destekli Ã¼retim".

---

### âœï¸ Ãœretim Nedir?

Ã–nce â€œÃ¼retimâ€ kÄ±smÄ±na odaklanalÄ±m, geri getirme kÄ±smÄ±nÄ± bir kenara bÄ±rakalÄ±m. Ãœretim, kullanÄ±cÄ± tarafÄ±ndan verilen bir sorguya (prompt) yanÄ±t olarak bÃ¼yÃ¼k dil modellerinin metin Ã¼retmesini ifade eder. Ancak bu modellerin istenmeyen bazÄ± davranÄ±ÅŸlarÄ± olabilir. Size bir anekdotla bunu aÃ§Ä±klamak istiyorum.

---

### ğŸª Bir Anekdot: Ay SayÄ±sÄ±

Ã‡ocuklarÄ±m bana ÅŸu soruyu sordu: GÃ¼neÅŸ sisteminde en Ã§ok uydusu olan gezegen hangisi? Ben de ÅŸÃ¶yle dedim: Harika bir soru! Ben de senin yaÅŸÄ±ndayken uzaya Ã§ok meraklÄ±ydÄ±m. (Tabii bu 30 yÄ±l Ã¶nceydi.) Ve ÅŸÃ¶yle dedim: Bir makalede okumuÅŸtum, cevap JÃ¼piter, 88 uydu. Yani bu benim cevabÄ±m.

Ama aslÄ±nda bu cevapta iki sorun var:

1. **Kaynak yok:** Makale dediÄŸim ÅŸeyin gerÃ§ekten var olup olmadÄ±ÄŸÄ±nÄ± bilmiyorum.
2. **GÃ¼ncellik sorunu:** Bu bilgi artÄ±k geÃ§erli deÄŸil olabilir.

Bu iki sorun, bÃ¼yÃ¼k dil modelleriyle etkileÅŸimde de sÄ±klÄ±kla gÃ¶rÃ¼len problemlerdir.

---

### ğŸ” GÃ¼ncel Bilgiye EriÅŸim

Peki ya cevabÄ±mÄ± vermeden Ã¶nce NASA gibi gÃ¼venilir bir kaynaktan araÅŸtÄ±rma yapsaydÄ±m? O zaman ÅŸÃ¶yle derdim: Cevap SatÃ¼rn, 146 uydu. Ãœstelik bu sayÄ± sÃ¼rekli deÄŸiÅŸiyor Ã§Ã¼nkÃ¼ bilim insanlarÄ± sÃ¼rekli yeni uydular keÅŸfediyor. BÃ¶ylece cevabÄ±mÄ± gÃ¼venilir bir temele dayandÄ±rmÄ±ÅŸ olurdum â€” uydurma deÄŸil, gÃ¼ncel ve doÄŸrulanabilir bilgiye dayalÄ±.

---

### ğŸ’¡ Peki LLM Ne YapardÄ±?

KullanÄ±cÄ± aynÄ± soruyu LLMâ€™e sorduÄŸunda, model yÃ¼ksek Ã¶zgÃ¼venle â€œJÃ¼piterâ€ cevabÄ±nÄ± verebilirdi. Fakat bu cevap yanlÄ±ÅŸ olurdu ve model bunun farkÄ±nda olmazdÄ±.

---

### ğŸ”— RAG Ne SaÄŸlar?

Åimdi RAGâ€™i ekleyelim. Bu ne demek? ArtÄ±k LLM yalnÄ±zca kendi eÄŸitildiÄŸi bilgilerle sÄ±nÄ±rlÄ± kalmaz, bir **iÃ§erik deposuna** eriÅŸir:

* Bu depo herkese aÃ§Ä±k (internet) olabilir.
* Ya da kapalÄ± (dokÃ¼man koleksiyonu, ÅŸirket politikalarÄ±) olabilir.

LLM, Ã¶nce iÃ§erik deposuna gidip kullanÄ±cÄ± sorgusuyla ilgili bilgileri getirir ve ancak o zaman bir yanÄ±t Ã¼retir.

---

### âš™ï¸ RAGâ€™in Ä°ÅŸleyiÅŸi

SÃ¼reÃ§ ÅŸÃ¶yle iÅŸler:

1. KullanÄ±cÄ± LLMâ€™e bir soru sorar.
2. Geleneksel model hemen cevap Ã¼retir.
3. RAG modeli ise Ã¶nce iÃ§erik getirir, sonra soruyla birleÅŸtirir, en son cevabÄ± Ã¼retir.

Yani artÄ±k istem Ã¼Ã§ parÃ§alÄ±dÄ±r: geri getirilen iÃ§erik + kullanÄ±cÄ± sorusu + Ã¼retim talimatÄ±. Ve sonuÃ§ta model cevabÄ±nÄ± neye dayandÄ±rdÄ±ÄŸÄ±nÄ± da gÃ¶sterebilir.

---

### âœ… RAG Ne TÃ¼r SorunlarÄ± Ã‡Ã¶zer?

Ä°ki bÃ¼yÃ¼k LLM sorununu Ã§Ã¶zer:

1. **GÃ¼ncellik sorunu:** Modeli yeniden eÄŸitmeye gerek kalmaz. Yeni bilgi geldikÃ§e iÃ§erik deposuna eklenir.
2. **Kaynak gÃ¶sterme:** Model artÄ±k yanÄ±tlarÄ±nÄ± birincil kaynaklara dayandÄ±rÄ±r. Bu, hayal Ã¼rÃ¼nÃ¼ (hallucination) cevaplarÄ±n ve veri sÄ±zÄ±ntÄ±sÄ±nÄ±n Ã¶nÃ¼ne geÃ§er.

---

### ğŸ¤·â€â™‚ï¸ Bilinmiyorsa â€œBilmiyorumâ€ Demek

Model, eÄŸer soruya verilecek gÃ¼venilir bir iÃ§erik yoksa, â€œbilmiyorumâ€ deme davranÄ±ÅŸÄ±nÄ± da Ã¶ÄŸrenebilir. Bu da kullanÄ±cÄ±yÄ± yanÄ±ltma riskini azaltÄ±r.

---

### âš ï¸ RAGâ€™in ZorluklarÄ±

Ancak bu durumun olumsuz yanÄ± da olabilir. EÄŸer geri getirme motoru yeterince kaliteli deÄŸilse, aslÄ±nda cevaplanabilecek sorular da cevapsÄ±z kalabilir. Bu nedenle IBM'deki bizler dahil birÃ§ok kiÅŸi hem geri getirme motorunu hem de Ã¼retim modelini iyileÅŸtirmek iÃ§in Ã§alÄ±ÅŸÄ±yoruz. BÃ¶ylece LLM, en kaliteli veriyle gÃ¼Ã§lÃ¼ ve doÄŸru yanÄ±tlar verebilir.

---

### ğŸ™ TeÅŸekkÃ¼rler

RAG hakkÄ±nda daha fazla ÅŸey Ã¶ÄŸrendiÄŸiniz iÃ§in teÅŸekkÃ¼rler.

---
