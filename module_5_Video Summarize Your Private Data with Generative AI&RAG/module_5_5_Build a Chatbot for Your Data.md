Ä°ÅŸte metnin TÃ¼rkÃ§eye Ã§evrilmiÅŸ ve baÅŸlÄ±klara ayrÄ±lmÄ±ÅŸ hali, her bÃ¶lÃ¼m iÃ§in emoji ile:

---

### ğŸ¤– Verileriniz Ä°Ã§in Sohbet Botu OluÅŸturun

**cognitiveclass.ai logosu**

**Tahmini sÃ¼re: 60 dakika**

---

### ğŸ§­ GiriÅŸ

Bu projede, kendi PDF dosyanÄ±z iÃ§in bir sohbet botu oluÅŸturacaksÄ±nÄ±z. Bunu yapmak iÃ§in popÃ¼ler bir web Ã§atÄ±sÄ± olan **Flask** ile bÃ¼yÃ¼k dil modelleri (LLM) ile Ã§alÄ±ÅŸmak iÃ§in kullanÄ±lan **LangChain** Ã§erÃ§evesini kullanacaksÄ±nÄ±z.

GeliÅŸtireceÄŸiniz sohbet botu, kullanÄ±cÄ±larla yalnÄ±zca metin Ã¼zerinden etkileÅŸime geÃ§mekle kalmayacak, aynÄ± zamanda belirli bir belgenin iÃ§eriÄŸini anlayacak ve bu iÃ§erikle ilgili sorularÄ± da yanÄ±tlayabilecektir.

---

### ğŸ¯ Proje Hedefi

AÅŸaÄŸÄ±daki demo baÄŸlantÄ±sÄ±na tÄ±klayarak, bu proje sonunda oluÅŸturacaÄŸÄ±nÄ±z nihai uygulamayÄ± deneyebilirsiniz!

Bu projenin sonunda:

* Sohbet botlarÄ±nÄ± daha derinlemesine anlayacaksÄ±nÄ±z.
* Flask ve Python kullanarak web uygulamasÄ± geliÅŸtirme becerileri kazanacaksÄ±nÄ±z.
* LangChain Ã§erÃ§evesini kullanarak geniÅŸ bir kullanÄ±cÄ± girdisi yelpazesine nasÄ±l yanÄ±t verileceÄŸini Ã¶ÄŸreneceksiniz.

Ve en Ã¶nemlisi: **KapsamlÄ± ve etkileyici bir sohbet botu uygulamasÄ± inÅŸa etmiÅŸ olacaksÄ±nÄ±z!**

![1748501281054](image/module_5_5_BuildaChatbotforYourData/1748501281054.png)


Ä°ÅŸte metnin TÃ¼rkÃ§eye Ã§evrilmiÅŸ ve baÅŸlÄ±klara ayrÄ±lmÄ±ÅŸ hali, her bÃ¶lÃ¼m iÃ§in emoji ile:

---

### ğŸ§± BaÅŸlamadan Ã–nce: KullanacaÄŸÄ±nÄ±z Temel BileÅŸenler

UygulamanÄ±zÄ± oluÅŸturmadan Ã¶nce, kullanacaÄŸÄ±nÄ±z ana bileÅŸenlere birlikte gÃ¶z atalÄ±m.

---

### ğŸ¤– Sohbet BotlarÄ± (Chatbots)

Sohbet botlarÄ±, insan benzeri konuÅŸmalar gerÃ§ekleÅŸtirmek Ã¼zere tasarlanmÄ±ÅŸ yazÄ±lÄ±m uygulamalarÄ±dÄ±r. KullanÄ±cÄ±lardan gelen metin girdilerine yanÄ±t verebilirler ve mÃ¼ÅŸteri hizmetleri, e-ticaret ve eÄŸitim gibi pek Ã§ok alanda yaygÄ±n olarak kullanÄ±lÄ±rlar.

Bu projede, yalnÄ±zca genel sohbet gerÃ§ekleÅŸtirmekle kalmayÄ±p aynÄ± zamanda belirli bir belgeye dayalÄ± sorularÄ± da yanÄ±tlayabilen bir sohbet botu geliÅŸtireceksiniz.

---

### ğŸ§  LangChain

LangChain, yapay zeka destekli dil uygulamalarÄ± geliÅŸtirmek iÃ§in Ã§ok yÃ¶nlÃ¼ bir araÃ§tÄ±r. Ã–nceden eÄŸitilmiÅŸ dil modellerinden yararlanarak metin alma, Ã¶zetleme, Ã§eviri gibi birÃ§ok iÅŸlev sunar.

Bu projede, LangChainâ€™i sohbet botunuza entegre ederek onun Ã§eÅŸitli kullanÄ±cÄ± girdilerini etkili ÅŸekilde anlamasÄ±nÄ± ve yanÄ±tlamasÄ±nÄ± saÄŸlayacaksÄ±nÄ±z.

---

### ğŸŒ Flask

Flask, Python iÃ§in hafif ve esnek bir web Ã§atÄ±sÄ±dÄ±r; basitliÄŸi ve hÄ±zÄ±yla bilinir. Web Ã§atÄ±sÄ±, web uygulamalarÄ±nÄ±n geliÅŸtirilmesini desteklemek Ã¼zere tasarlanmÄ±ÅŸ yazÄ±lÄ±m yapÄ±sÄ±dÄ±r ve web sunucularÄ± oluÅŸturmayÄ±, HTTP istek ve yanÄ±tlarÄ±nÄ± yÃ¶netmeyi saÄŸlar.

Sohbet botunuzun sunucu tarafÄ±nÄ± (backend) oluÅŸturmak iÃ§in Flask kullanacaksÄ±nÄ±z. Bu, kullanÄ±cÄ±dan gelen mesajlarÄ±n iÅŸlenmesi ve uygun yanÄ±tlarÄ±n geri gÃ¶nderilmesi iÅŸlemlerini kapsar.

---

### ğŸ§­ Flaskâ€™te Rotalar (Routes)

Rotalar, web geliÅŸtirmede temel bir bileÅŸendir. UygulamanÄ±z bir istemciden (genellikle bir web tarayÄ±cÄ±sÄ±) bir istek aldÄ±ÄŸÄ±nda, bu isteÄŸin nasÄ±l ele alÄ±nacaÄŸÄ±nÄ± bilmesi gerekir. Ä°ÅŸte burada â€œroutingâ€ devreye girer.

Flaskâ€™te, `@app.route` dekoratÃ¶rÃ¼ kullanÄ±larak URL rotalarÄ±na iÅŸlevler baÄŸlanÄ±r. Bir kullanÄ±cÄ± bu URLâ€™yi ziyaret ettiÄŸinde, ilgili iÅŸlev Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r.

Sohbet botu projenizde, kullanÄ±cÄ± mesajlarÄ±nÄ± taÅŸÄ±yan POST isteklerini ele almak ve belge verilerini iÅŸlemek iÃ§in rotalarÄ± kullanacaksÄ±nÄ±z.

---

### ğŸ’» HTML - CSS - JavaScript

Size HTML, CSS ve JavaScript kullanÄ±larak oluÅŸturulmuÅŸ, kullanÄ±ma hazÄ±r bir sohbet botu arayÃ¼zÃ¼ sunulmuÅŸtur.

* **HTML:** Web iÃ§eriÄŸini yapÄ±landÄ±rÄ±r.
* **CSS:** GÃ¶rsel biÃ§imlendirme saÄŸlar.
* **JavaScript:** EtkileÅŸim katar.

Bu teknolojiler, gÃ¶rsel olarak hoÅŸ ve etkileÅŸimli bir sohbet arayÃ¼zÃ¼ oluÅŸturur.

---

### ğŸ–¼ï¸ ArayÃ¼z GÃ¶rseli

Ä°ÅŸte sohbet botu arayÃ¼zÃ¼nden bir anlÄ±k gÃ¶rÃ¼ntÃ¼:

*(Not: GÃ¶rsel belge iÃ§inde tanÄ±mlandÄ±ÄŸÄ± iÃ§in burada yer almamaktadÄ±r.)*

---

![1748501329993](image/module_5_5_BuildaChatbotforYourData/1748501329993.png)

---

### ğŸ¯ Ã–ÄŸrenme Hedefleri

Bu projenin sonunda aÅŸaÄŸÄ±dakileri yapabiliyor olacaksÄ±nÄ±z:

* LangChain ve yapay zeka uygulamalarÄ±nÄ±n temellerini aÃ§Ä±klamak
* Python Flask kullanarak bir asistan geliÅŸtirmek iÃ§in geliÅŸtirme ortamÄ± kurmak
* KullanÄ±cÄ±lardan gelen PDF dosyalarÄ±nÄ± algÄ±layabilmesi iÃ§in asistanÄ±nÄ±za PDF yÃ¼kleme iÅŸlevi eklemek
* AsistanÄ± aÃ§Ä±k kaynak modellerle entegre ederek kullanÄ±cÄ± isteklerini anlayÄ±p yanÄ±t verebilecek yÃ¼ksek seviyede zeka kazandÄ±rmak
* *(Ä°steÄŸe baÄŸlÄ±)* PDF asistanÄ±nÄ±zÄ± daha geniÅŸ bir kullanÄ±cÄ± kitlesinin eriÅŸebilmesi iÃ§in bir web sunucusuna daÄŸÄ±tmak

---

### ğŸ“š Ã–n Bilgiler (Gereksinimler)

HTML/CSS, JavaScript ve Python'un temellerine dair bilgi sahibi olmanÄ±z faydalÄ± olabilir, ancak zorunlu deÄŸildir. Bu laboratuvar boyunca her adÄ±m ve kod bÃ¶lÃ¼mÃ¼ kapsamlÄ± biÃ§imde aÃ§Ä±klanacaktÄ±r.

---

### ğŸš€ Haydi BaÅŸlayalÄ±m!

ArtÄ±k gerekli temele sahipsiniz; projenize baÅŸlamaya hazÄ±rsÄ±nÄ±z!

---


Ä°ÅŸte metnin TÃ¼rkÃ§eye Ã§evrilmiÅŸ ve baÅŸlÄ±klara ayrÄ±lmÄ±ÅŸ hali, her bÃ¶lÃ¼m iÃ§in emoji ile:

---

### ğŸ’¬ KullanÄ±cÄ± ArayÃ¼zÃ¼nÃ¼ Kurma ve Anlama

Bu projedeki hedef, iletiÅŸime olanak tanÄ±yan bir arayÃ¼ze sahip bir sohbet botu oluÅŸturmaktÄ±r.

---

### âš™ï¸ OrtamÄ± Kurma

Ä°lk olarak aÅŸaÄŸÄ±daki komutlarÄ± Ã§alÄ±ÅŸtÄ±rarak geliÅŸtirme ortamÄ±nÄ± kuruyoruz:

```bash
pip3 install virtualenv 
virtualenv my_env # my_env adÄ±nda sanal ortam oluÅŸtur
source my_env/bin/activate # sanal ortamÄ± etkinleÅŸtir
```

---

### ğŸ–¥ï¸ ArayÃ¼z ve Genel YapÄ±

Frontend kÄ±smÄ± **HTML, CSS ve JavaScript** kullanacaktÄ±r. KullanÄ±cÄ± arayÃ¼zÃ¼, internette sÄ±kÃ§a gÃ¶rdÃ¼ÄŸÃ¼nÃ¼z sohbet botlarÄ±na benzer olacaktÄ±r. ArayÃ¼z iÃ§in gereken kodlar size saÄŸlanmÄ±ÅŸtÄ±r. Bu rehberli projede odak noktanÄ±z, bu arayÃ¼zÃ¼ arka uca (backend) baÄŸlamak olacaktÄ±r. Backend, kullanÄ±cÄ± belgelerinin yÃ¼klenmesini ve LLM modeli ile etkileÅŸimi yÃ¶netecektir.

Bu proje sayesinde frontend ve backendâ€™in nasÄ±l etkileÅŸime girdiÄŸini anlayacak, Ã¶nemli bileÅŸenlerin nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± Ã¶ÄŸrenecek ve basit bir web sayfasÄ± oluÅŸturma konusunda net bir kavrayÄ±ÅŸa sahip olacaksÄ±nÄ±z.

---

### ğŸ“ Proje DosyalarÄ±nÄ± Ä°ndirme

AÅŸaÄŸÄ±daki komutlarÄ± Ã§alÄ±ÅŸtÄ±rarak projeyi bilgisayarÄ±nÄ±za indirin, uygun bir isim verin ve o dizine geÃ§in:

```bash
git clone https://github.com/ibm-developer-skills-network/wbphl-build_own_chatbot_without_open_ai.git
mv wbphl-build_own_chatbot_without_open_ai build_chatbot_for_your_data
cd build_chatbot_for_your_data
```

---

### ğŸ“¦ Gerekli Paketlerin Kurulumu

Projeye Ã¶zel baÄŸÄ±mlÄ±lÄ±klarÄ± yÃ¼klemek iÃ§in:

```bash
pip install -r requirements.txt
```

LangChain bileÅŸenini eklemek iÃ§in:

```bash
pip install langchain-community
```

â˜• Bir kahve alÄ±n! Bu kurulum 5â€“10 dakika sÃ¼rebilir. Ancak proje Ã¼zerinde Ã§alÄ±ÅŸmaya devam edebilirsiniz.

```
          )  (
         (   ) )
          ) ( (
        _______)_
     .-'---------|  
    ( C|/\/\/\/\/|
     '-./\/\/\/\/|
       '_________'
        '-------'
```

---

### ğŸŒ Frontend DosyalarÄ±nÄ±n YapÄ±sÄ±

#### ğŸ“„ `index.html`

Web arayÃ¼zÃ¼nÃ¼n yapÄ±sÄ±nÄ± ve dÃ¼zenini belirler. DÄ±ÅŸ kÃ¼tÃ¼phaneleri iÃ§erir:

* JQuery
* Bootstrap
* FontAwesome ikonlarÄ±

  AyrÄ±ca stil (style.css) ve etkileÅŸim (script.js) dosyalarÄ±nÄ± dahil eder.

#### ğŸ¨ `style.css`

Sayfadaki bileÅŸenlerin gÃ¶rsel gÃ¶rÃ¼nÃ¼mÃ¼nÃ¼ Ã¶zelleÅŸtirir. AyrÄ±ca CSS *keyframes* ile yÃ¼kleme animasyonunu kontrol eder. Keyframes, bir animasyonun zaman iÃ§indeki stil deÄŸiÅŸimini tanÄ±mlamak iÃ§in kullanÄ±lÄ±r ve dinamik geÃ§iÅŸler saÄŸlar.

#### ğŸ“œ `script.js`

SayfanÄ±n etkileÅŸimini ve iÅŸlevselliÄŸini yÃ¶netir. Ä°Ã§eriÄŸinde:

* KaranlÄ±k/aydÄ±nlÄ±k mod geÃ§iÅŸi
* Mesaj gÃ¶nderme
* Yeni mesajlarÄ± ekranda gÃ¶sterme
* Ses kaydÄ± Ã¶zelliÄŸi

  gibi temel iÅŸlevler yer alÄ±r.



---

### ğŸ§  Workerâ€™Ä± Anlamak: Belge Ä°ÅŸleme ve KonuÅŸma YÃ¶netimi (BÃ¶lÃ¼m 1)

Bu bÃ¶lÃ¼mde, sohbet botu uygulamasÄ±nÄ±n bir parÃ§asÄ± olan `worker.py` dosyasÄ±nÄ± inceleyeceÄŸiz. Bu dosya, kullanÄ±cÄ± mesajlarÄ±nÄ± ve belgeleri iÅŸlemekle ilgilidir. Python dilinde yazÄ±lmÄ±ÅŸ olan **LangChain** kÃ¼tÃ¼phanesini kullanÄ±r ve konuÅŸma tabanlÄ± yapay zeka uygulamalarÄ± geliÅŸtirmeye yÃ¶neliktir.

---

### âš™ï¸ worker.pyâ€™nin GÃ¶revleri

Bu betik (script) aÅŸaÄŸÄ±daki gÃ¶revlerden sorumludur:

* **Dil modelini kurmak:** KullanÄ±lacak bÃ¼yÃ¼k dil modelini baÅŸlatmak ve yapÄ±landÄ±rmak
* **PDF belgelerini iÅŸlemek:** Belgeleri, konuÅŸmaya dayalÄ± bilgi alÄ±mÄ± iÃ§in uygun bir formata dÃ¶nÃ¼ÅŸtÃ¼rmek
* **KullanÄ±cÄ± girdilerini iÅŸlemek:** KullanÄ±cÄ±nÄ±n gÃ¶nderdiÄŸi mesajlara, iÅŸlenmiÅŸ belgeler Ã¼zerinden yanÄ±t Ã¼retmek

---

### ğŸ§¾ worker.py Ä°Ã§eriÄŸine Genel BakÄ±ÅŸ

`worker.py` dosyasÄ±, belirli bir PDF belgesinin iÃ§eriÄŸine dayalÄ± olarak sorularÄ± yanÄ±tlayabilen bir konuÅŸma arayÃ¼zÃ¼ saÄŸlamak amacÄ±yla tasarlanmÄ±ÅŸtÄ±r.

---

### ğŸ› ï¸ GÃ¶reviniz

Bu projede gÃ¶reviniz, `worker.py` dosyasÄ±ndaki aÃ§Ä±klama satÄ±rlarÄ±nÄ±n (yorumlarÄ±n) altÄ±na uygun Python kodlarÄ±nÄ± yerleÅŸtirmektir. Kodlar sayesinde sohbet botu:

* PDF dosyasÄ±nÄ± okuyacak
* Ä°Ã§eriÄŸini bÃ¶lerek analiz edilebilir hale getirecek
* Bu iÃ§erikle etkileÅŸim kurabilecek ÅŸekilde model kuracak
* KullanÄ±cÄ±dan gelen istemleri anlayarak yanÄ±tlar oluÅŸturacaktÄ±r

---

HazÄ±rsanÄ±z, ÅŸimdi `worker.py` dosyasÄ±nÄ±n bÃ¶lÃ¼mlerini detaylÄ±ca ele alacaÄŸÄ±z!

![1748501543321](image/module_5_5_BuildaChatbotforYourData/1748501543321.png)



### ğŸ—‚ï¸ SÃ¼recin Genel GÃ¶rÃ¼nÃ¼mÃ¼: Belge Ä°ÅŸleme ve Bilgi Getirme

Bu diyagram, belge iÅŸleme ve bilgi getirme sÃ¼recini gÃ¶stermektedir. SÃ¼reÃ§, bÃ¼yÃ¼k bir dil modeli (LLM) ile entegre bir ÅŸekilde Ã§alÄ±ÅŸarak, **soru yanÄ±tlama** gÃ¶revini kolaylaÅŸtÄ±rÄ±r.

---

### ğŸ› ï¸ TÃ¼m Ä°ÅŸlem Nerede GerÃ§ekleÅŸiyor?

Bu iÅŸlemlerin tamamÄ± **`worker.py`** dosyasÄ±nda gerÃ§ekleÅŸir.

Bu dosya; PDF belgesinin iÅŸlenmesinden, bilgi getirme adÄ±mÄ±na ve modelin kullanÄ±cÄ± sorularÄ±na yanÄ±t Ã¼retmesine kadar olan tÃ¼m adÄ±mlarÄ± kapsar.

---

ğŸ“Œ *GÃ¶rsel kaynak baÄŸlantÄ±sÄ±: image credit link*

---


Ä°ÅŸte aÃ§Ä±klamalÄ± metnin TÃ¼rkÃ§eye Ã§evrilmiÅŸ ve baÅŸlÄ±klara ayrÄ±lmÄ±ÅŸ hali ile birlikte, istenen Python kodu da doÄŸru ÅŸekilde tamamlanmÄ±ÅŸ olarak verilmiÅŸtir:

---

### âš™ï¸ BaÅŸlatma: `init_llm()` Fonksiyonu

Bu fonksiyon, yapay zeka dil modelini (Watsonx LLM) ve gÃ¶mlemeleri (embeddings) baÅŸlatmak iÃ§in kullanÄ±lÄ±r. `worker.py` dosyasÄ±nda tanÄ±mlÄ±dÄ±r ve ÅŸu iÅŸlemleri gerÃ§ekleÅŸtirir:

---

### ğŸ” Kimlik Bilgilerinin AyarlanmasÄ±

Watsonx platformuna eriÅŸim iÃ§in servis URLâ€™si ve kimlik doÄŸrulama belirteci (token) iÃ§eren bir sÃ¶zlÃ¼k (dictionary) oluÅŸturulur.

---

### ğŸ›ï¸ Parametrelerin YapÄ±landÄ±rÄ±lmasÄ±

* **max_new_tokens:** Ãœretilecek maksimum token sayÄ±sÄ± (256)
* **temperature:** Rastgelelik dÃ¼zeyi (0.1)

---

### ğŸ§  Modelin BaÅŸlatÄ±lmasÄ±

* Meta tarafÄ±ndan geliÅŸtirilen **Llama 3** modeli kullanÄ±lmaktadÄ±r.
* IBM Watsonx ortamÄ±nda, `"skills-network"` projesine Ã¶zel olarak tanÄ±mlanmÄ±ÅŸtÄ±r.

---

### ğŸ§¬ GÃ¶mlemelerin (Embeddings) BaÅŸlatÄ±lmasÄ±

Metni vektÃ¶r temsiline dÃ¶nÃ¼ÅŸtÃ¼rmek iÃ§in HuggingFaceâ€™in Ã¶nceden eÄŸitilmiÅŸ modeli kullanÄ±lÄ±r.

---

### âœ… TamamlanmÄ±ÅŸ Kod

AÅŸaÄŸÄ±da `worker.py` dosyasÄ±na yerleÅŸtirmeniz gereken doÄŸru ve eksiksiz kod verilmiÅŸtir:

```python
# placeholder for Watsonx_API and Project_id in case you need to use the code outside this environment
Watsonx_API = "Your WatsonX API"
Project_id = "Your Project ID"

# Function to initialize the language model and its embeddings
def init_llm():
    global llm_hub, embeddings
    logger.info("Initializing WatsonxLLM and embeddings...")
  
    # Llama Model Configuration
    MODEL_ID = "meta-llama/llama-3-3-70b-instruct"
    WATSONX_URL = "https://us-south.ml.cloud.ibm.com"
    PROJECT_ID = "skills-network"
  
    # Model parameters
    model_parameters = {
        "max_new_tokens": 256,
        "temperature": 0.1,
    }
  
    # Initialize Llama LLM using the updated WatsonxLLM API
    llm_hub = WatsonxLLM(
        model_id=MODEL_ID,
        url=WATSONX_URL,
        project_id=PROJECT_ID,
        params=model_parameters
    )
    logger.debug("WatsonxLLM initialized: %s", llm_hub)
  
    # Initialize embeddings using a pre-trained model to represent the text data.
    embeddings = HuggingFaceInstructEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2", 
        model_kwargs={"device": DEVICE}
    )
    logger.debug("Embeddings initialized with model device: %s", DEVICE)
```

---

Bu yapÄ±, `worker.py` iÃ§inde hem Watsonx LLM modelini hem de metin gÃ¶mlemelerini baÅŸarÄ±yla baÅŸlatmanÄ±zÄ± saÄŸlar. DevamÄ±nda belge iÅŸleme (`process_document`) ve kullanÄ±cÄ± sorgusu iÅŸleme (`process_prompt`) fonksiyonlarÄ±yla birlikte kullanÄ±lacaktÄ±r.

### ğŸ§  Workerâ€™Ä± Anlamak â€“ BÃ¶lÃ¼m 2

---

### ğŸ“„ Belge Ä°ÅŸleme: `process_document` Fonksiyonu

**Belge iÅŸleme:** `process_document` fonksiyonu, PDF belgelerini iÅŸlemekten sorumludur.

Bu iÅŸlem, belgeyi yÃ¼klemek, parÃ§alara ayÄ±rmak, gÃ¶mlemelerle bir vektÃ¶r deposu oluÅŸturmak ve ardÄ±ndan bu veritabanÄ±nÄ± kullanarak bir bilgi getirme (retrieval) sistemi kurmaktan oluÅŸur. Bu sistem, belgenin iÃ§eriÄŸine dayalÄ± sorularÄ± yanÄ±tlamak iÃ§in bir **ConversationalRetrievalChain** oluÅŸturur.

---

#### ğŸ“¥ Belge YÃ¼kleme

PDF belgesi, `PyPDFLoader` sÄ±nÄ±fÄ± kullanÄ±larak yÃ¼klenir. Bu sÄ±nÄ±f, belge yolunu argÃ¼man olarak alÄ±r.

**(YapÄ±lacak alÄ±ÅŸtÄ±rma: `PyPDFLoader(...)` ifadesini `loader` deÄŸiÅŸkenine ata)**

---

#### âœ‚ï¸ Belgeyi ParÃ§alama

YÃ¼klenen belge, `RecursiveCharacterTextSplitter` sÄ±nÄ±fÄ± kullanÄ±larak parÃ§alara ayrÄ±lÄ±r.

**chunk_size** ve **chunk_overlap** gibi parametreler belirtilebilir.

**(YapÄ±lacak alÄ±ÅŸtÄ±rma: `RecursiveCharacterTextSplitter(...)` ifadesini `text_splitter` deÄŸiÅŸkenine ata)**

---

#### ğŸ“¦ VektÃ¶r Deposu OluÅŸturma

Belge parÃ§alarÄ±ndan, dil modeli gÃ¶mlemeleri kullanÄ±larak bir vektÃ¶r deposu (Chroma) oluÅŸturulur. Bu, bir tÃ¼r indeks iÅŸlevi gÃ¶rÃ¼r.

---

#### ğŸ” Bilgi Getirme Sistemi Kurulumu

OluÅŸturulan vektÃ¶r deposu ile bir bilgi getirme sistemi kurulur.

Bu sistem, belgedeki iÃ§eriklere dayanarak sorularÄ± yanÄ±tlayan **ConversationalRetrievalChain** oluÅŸturur.

---

### âœ… Kod (Eksiksiz ve TercÃ¼meli)

```python


# Bir PDF belgesini iÅŸlemek iÃ§in fonksiyon
def process_document(document_path):
    global conversation_retrieval_chain

    logger.info("Belge ÅŸu yoldan yÃ¼kleniyor: %s", document_path)

    # Belgeyi yÃ¼kle
    loader = PyPDFLoader(document_path)  # ---> belgeyi PyPDFLoader ile yÃ¼kle
    documents = loader.load()
    logger.debug("%d belge yÃ¼klendi", len(documents))

    # Belgeyi parÃ§alara ayÄ±r (chunk_size=1024, chunk_overlap=64)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)
    texts = text_splitter.split_documents(documents)
    logger.debug("Belge %d metin parÃ§asÄ±na bÃ¶lÃ¼ndÃ¼", len(texts))

    # Metin parÃ§alarÄ±ndan Chroma ile bir gÃ¶mleme veritabanÄ± oluÅŸtur
    logger.info("Chroma vektÃ¶r deposu belgelerden baÅŸlatÄ±lÄ±yor...")
    db = Chroma.from_documents(texts, embedding=embeddings)
    logger.debug("Chroma vektÃ¶r deposu baÅŸlatÄ±ldÄ±.")

    # (Ä°steÄŸe baÄŸlÄ±) Chroma'daki mevcut koleksiyonlarÄ± gÃ¼nlÃ¼ÄŸe yaz
    try:
        collections = db._client.list_collections()
        logger.debug("Chroma'daki mevcut koleksiyonlar: %s", collections)
    except Exception as e:
        logger.warning("Chroma'dan koleksiyonlar alÄ±namadÄ±: %s", e)

    # LLM ve retriever kullanarak QA (soru-cevap) zinciri oluÅŸtur
    conversation_retrieval_chain = RetrievalQA.from_chain_type(
        llm=llm_hub,
        chain_type="stuff",
        retriever=db.as_retriever(search_type="mmr", search_kwargs={'k': 6, 'lambda_mult': 0.25}),
        return_source_documents=False,
        input_key="question"
        # chain_type_kwargs={"prompt": prompt}  # EÄŸer Ã¶zel prompt kullanÄ±yorsanÄ±z bu satÄ±rÄ± aÃ§Ä±n
    )
    logger.info("RetrievalQA zinciri baÅŸarÄ±yla oluÅŸturuldu.")
```

# CÃ¶zÃ¼m


### ğŸ“„ PDF Belgesini Ä°ÅŸleyen Fonksiyon (TÃ¼rkÃ§e AÃ§Ä±klamalÄ±)

```python
# Bir PDF belgesini iÅŸlemek iÃ§in fonksiyon
def process_document(document_path):
    global conversation_retrieval_chain

    logger.info("Belge ÅŸu yoldan yÃ¼kleniyor: %s", document_path)
  
    # Belgeyi yÃ¼kle
    loader = PyPDFLoader(document_path)
    documents = loader.load()
    logger.debug("%d belge yÃ¼klendi", len(documents))

    # Belgeyi parÃ§alara ayÄ±r (chunk_size=1024, chunk_overlap=64)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)
    texts = text_splitter.split_documents(documents)
    logger.debug("Belge %d metin parÃ§asÄ±na bÃ¶lÃ¼ndÃ¼", len(texts))

    # Metin parÃ§alarÄ±ndan Chroma ile bir gÃ¶mleme veritabanÄ± oluÅŸtur
    logger.info("Belgelerden Chroma vektÃ¶r deposu baÅŸlatÄ±lÄ±yor...")
    db = Chroma.from_documents(texts, embedding=embeddings)
    logger.debug("Chroma vektÃ¶r deposu baÅŸlatÄ±ldÄ±.")

    # (Ä°steÄŸe baÄŸlÄ±) Chroma'daki mevcut koleksiyonlarÄ± gÃ¼nlÃ¼ÄŸe yaz
    try:
        collections = db._client.list_collections()  # _client dahili olabilir
        logger.debug("Chroma'daki mevcut koleksiyonlar: %s", collections)
    except Exception as e:
        logger.warning("Chroma koleksiyonlarÄ± alÄ±namadÄ±: %s", e)

    # LLM ve retriever kullanarak soru-cevap zinciri oluÅŸtur
    conversation_retrieval_chain = RetrievalQA.from_chain_type(
        llm=llm_hub,
        chain_type="stuff",
        retriever=db.as_retriever(search_type="mmr", search_kwargs={'k': 6, 'lambda_mult': 0.25}),
        return_source_documents=False,
        input_key="question"
        # chain_type_kwargs={"prompt": prompt}  # Ã–zel bir prompt kullanÄ±yorsanÄ±z bu satÄ±rÄ± aÃ§Ä±n
    )
    logger.info("RetrievalQA zinciri baÅŸarÄ±yla oluÅŸturuldu.")
```


### ğŸ’¬ Ä°stek Ä°ÅŸleme (`process_prompt` fonksiyonu)

**Ä°stek iÅŸleme:**

Bu fonksiyon, kullanÄ±cÄ±nÄ±n bir istemini (sorusunu) iÅŸler, daha Ã¶nce iÅŸlenmiÅŸ PDF belgesinin iÃ§eriÄŸine dayalÄ± bir yanÄ±t alÄ±r ve sohbet geÃ§miÅŸini gÃ¼ncel tutar.

#### Ä°ÅŸlevleri:

* KullanÄ±cÄ±dan gelen istemi ve mevcut sohbet geÃ§miÅŸini `ConversationalRetrievalChain` nesnesine aktarÄ±r.
* Modelden alÄ±nan yanÄ±tla birlikte istemi sohbet geÃ§miÅŸine ekler.
* Modelin oluÅŸturduÄŸu yanÄ±tÄ± dÃ¶ndÃ¼rÃ¼r.

---

### âœ… Kod Ä°skeleti (TamamlanmÄ±ÅŸ ve TÃ¼rkÃ§e AÃ§Ä±klamalÄ±)

```python
# Bir kullanÄ±cÄ± istemini iÅŸlemek iÃ§in fonksiyon
def process_prompt(prompt):
    global conversation_retrieval_chain
    global chat_history

    logger.info("Ä°stem iÅŸleniyor: %s", prompt)

    # Modeli invoke() yÃ¶ntemi ile sorgula
    output = conversation_retrieval_chain.invoke({
        "question": prompt,
        "chat_history": chat_history
    })
  
    # Modelin yanÄ±tÄ±nÄ± al
    answer = output["result"]
    logger.debug("Model yanÄ±tÄ±: %s", answer)

    # Sohbet geÃ§miÅŸine istem ve yanÄ±tÄ± ekle
    chat_history.append((prompt, answer))
    logger.debug("Sohbet geÃ§miÅŸi gÃ¼ncellendi. Toplam alÄ±ÅŸveriÅŸ sayÄ±sÄ±: %d", len(chat_history))

    # YanÄ±tÄ± dÃ¶ndÃ¼r
    return answer
```


### ğŸ’¬ KullanÄ±cÄ± Ä°steÄŸini Ä°ÅŸleme Fonksiyonu â€“ TÃ¼rkÃ§e AÃ§Ä±klamalÄ±

```python
# Bir kullanÄ±cÄ± istemini (sorusunu) iÅŸlemek iÃ§in fonksiyon
def process_prompt(prompt):
    global conversation_retrieval_chain  # Model zincirine eriÅŸim
    global chat_history  # Sohbet geÃ§miÅŸine eriÅŸim

    logger.info("Ä°stem iÅŸleniyor: %s", prompt)

    # Modeli invoke() yÃ¶ntemiyle sorgula, Ã¶nceki sohbet geÃ§miÅŸiyle birlikte
    output = conversation_retrieval_chain.invoke({
        "question": prompt,
        "chat_history": chat_history
    })

    # Modelin verdiÄŸi cevabÄ± al
    answer = output["result"]
    logger.debug("Model yanÄ±tÄ±: %s", answer)

    # Sohbet geÃ§miÅŸini gÃ¼ncelle: (soru, yanÄ±t) Ã§iftini ekle
    chat_history.append((prompt, answer))
    logger.debug("Sohbet geÃ§miÅŸi gÃ¼ncellendi. Toplam alÄ±ÅŸveriÅŸ sayÄ±sÄ±: %d", len(chat_history))

    # YanÄ±tÄ± dÃ¶ndÃ¼r
    return answer
```


### ğŸŒ Global DeÄŸiÅŸkenler

* `llm` ve `llm_embeddings`: Dil modelini ve bu modelin gÃ¶mlemelerini (embeddings) tutar.
* `conversation_retrieval_chain` ve `chat_history`: Sohbet zinciri ve geÃ§miÅŸini saklamak iÃ§in kullanÄ±lÄ±r.
* Bu deÄŸiÅŸkenlere fonksiyonlar iÃ§inden eriÅŸebilmek iÃ§in `global` ifadesi kullanÄ±lÄ±r (`init_llm`, `process_document`, `process_prompt` fonksiyonlarÄ±nda).
* Bu sayede deÄŸiÅŸkenlerde yapÄ±lan deÄŸiÅŸiklikler fonksiyon dÄ±ÅŸÄ±nda da geÃ§erli olur ve programÄ±n genel durumunu etkiler.

---

### ğŸ§¾ Tam worker.py DosyasÄ±nÄ±n TÃ¼rkÃ§eye Ã‡evrilmiÅŸ Hali (Kod)

```python
import os
import torch
import logging

# GÃ¼nlÃ¼k kayÄ±t yapÄ±landÄ±rmasÄ±
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

from langchain_core.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_community.embeddings import HuggingFaceInstructEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_ibm import WatsonxLLM

# GPU kullanÄ±labiliyorsa cihazÄ± belirle
DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"

# Global deÄŸiÅŸkenler
conversation_retrieval_chain = None
chat_history = []
llm_hub = None
embeddings = None

# Dil modelini ve gÃ¶mlemeleri baÅŸlatan fonksiyon
def init_llm():
    global llm_hub, embeddings
    logger.info("WatsonxLLM ve embeddings baÅŸlatÄ±lÄ±yor...")

    MODEL_ID = "meta-llama/llama-3-3-70b-instruct"
    WATSONX_URL = "https://us-south.ml.cloud.ibm.com"
    PROJECT_ID = "skills-network"

    model_parameters = {
        "max_new_tokens": 256,
        "temperature": 0.1,
    }

    llm_hub = WatsonxLLM(
        model_id=MODEL_ID,
        url=WATSONX_URL,
        project_id=PROJECT_ID,
        params=model_parameters
    )
    logger.debug("WatsonxLLM baÅŸlatÄ±ldÄ±: %s", llm_hub)

    embeddings = HuggingFaceInstructEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={"device": DEVICE}
    )
    logger.debug("Embeddings baÅŸlatÄ±ldÄ±, cihaz: %s", DEVICE)

# PDF belgesini iÅŸleyen fonksiyon
def process_document(document_path):
    global conversation_retrieval_chain
    logger.info("Belge ÅŸu yoldan yÃ¼kleniyor: %s", document_path)

    loader = PyPDFLoader(document_path)
    documents = loader.load()
    logger.debug("%d belge yÃ¼klendi", len(documents))

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)
    texts = text_splitter.split_documents(documents)
    logger.debug("Belge %d metin parÃ§asÄ±na bÃ¶lÃ¼ndÃ¼", len(texts))

    logger.info("Chroma vektÃ¶r deposu baÅŸlatÄ±lÄ±yor...")
    db = Chroma.from_documents(texts, embedding=embeddings)
    logger.debug("Chroma vektÃ¶r deposu baÅŸlatÄ±ldÄ±.")

    try:
        collections = db._client.list_collections()
        logger.debug("Chroma koleksiyonlarÄ±: %s", collections)
    except Exception as e:
        logger.warning("Chroma koleksiyonlarÄ± alÄ±namadÄ±: %s", e)

    conversation_retrieval_chain = RetrievalQA.from_chain_type(
        llm=llm_hub,
        chain_type="stuff",
        retriever=db.as_retriever(search_type="mmr", search_kwargs={'k': 6, 'lambda_mult': 0.25}),
        return_source_documents=False,
        input_key="question"
    )
    logger.info("RetrievalQA zinciri baÅŸarÄ±yla oluÅŸturuldu.")

# KullanÄ±cÄ±dan gelen istemi iÅŸleyen fonksiyon
def process_prompt(prompt):
    global conversation_retrieval_chain
    global chat_history
    logger.info("Ä°stem iÅŸleniyor: %s", prompt)

    output = conversation_retrieval_chain.invoke({
        "question": prompt,
        "chat_history": chat_history
    })
    answer = output["result"]
    logger.debug("Model yanÄ±tÄ±: %s", answer)

    chat_history.append((prompt, answer))
    logger.debug("Sohbet geÃ§miÅŸi gÃ¼ncellendi. Toplam deÄŸiÅŸim sayÄ±sÄ±: %d", len(chat_history))

    return answer

# BaÅŸlat
init_llm()
logger.info("LLM ve embeddings baÅŸlatma tamamlandÄ±.")
```



### ğŸš€ CloudIDE'de UygulamayÄ± Ã‡alÄ±ÅŸtÄ±rma

Sohbet botunuzu uygulamak iÃ§in Ã¶ncelikle `server.py` dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rmanÄ±z gerekir:

---

### â–¶ï¸ AdÄ±m 1: Sunucuyu BaÅŸlatma

```bash
python3 server.py
```

---

### ğŸ“Ÿ Terminal Ã‡Ä±ktÄ±sÄ±

Terminalde ÅŸu ÅŸekilde bir Ã§Ä±ktÄ± gÃ¶rmelisiniz:

```
 * Running on http://127.0.0.1:8000/ (Press CTRL+C to quit)
```

Bu, sunucunun Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶sterir.

![1748502496365](image/module_5_5_BuildaChatbotforYourData/1748502496365.png)

---

### ğŸŒ TarayÄ±cÄ±da AÃ§ma

 **EÄŸer uygulamayÄ± yerel olarak Ã§alÄ±ÅŸtÄ±rÄ±yorsanÄ±z** , tarayÄ±cÄ±nÄ±zÄ± aÃ§Ä±n ve ÅŸu adresi yazÄ±n:

```
http://127.0.0.1:8000
```

---

### ğŸ–±ï¸ UygulamanÄ±zÄ± AÃ§Ä±n

AÅŸaÄŸÄ±daki butona tÄ±klayarak uygulamanÄ±zÄ± yeni bir pencerede aÃ§abilirsiniz:

> ğŸ’¡ *Yeni bir pencere aÃ§Ä±lacak ve sohbet botu arayÃ¼zÃ¼ gÃ¶rÃ¼ntÃ¼lenecek.*
>
> ![1748502513176](image/module_5_5_BuildaChatbotforYourData/1748502513176.png)




### ğŸ‰ Harika Ä°ÅŸ Ã‡Ä±kardÄ±nÄ±z, Projenizi TamamladÄ±nÄ±z!

Sohbet botu uygulamanÄ±z baÅŸarÄ±yla Ã§alÄ±ÅŸtÄ±!

---

### ğŸ“‚ Devam Etmek Ä°stiyorsanÄ±z

EÄŸer:

* `server.py` dosyasÄ±nÄ± ve JavaScript dosyalarÄ±nÄ± daha iyi anlamak,
* UygulamayÄ± Docker kullanarak daÄŸÄ±tÄ±m iÃ§in containerize etmek

istiyorsanÄ±z, projeye devam edebilirsiniz.

---

### ğŸ§ª Uygulamayla Denemeler YaptÄ±ktan Sonra

UygulamayÄ± Ã§alÄ±ÅŸtÄ±rdÄ±ktan sonra biraz test edip keÅŸfettikten sonra:

ğŸ’» **Terminali durdurmak iÃ§in:**

Mac veya Windows'ta ÅŸu tuÅŸlara aynÄ± anda basÄ±n:

```text
Ctrl + C  (Mac'te Control + C veya ^ + C olarak da geÃ§er)
```

Bu iÅŸlem konteyneri veya sunucuyu durdurur ve projeye kaldÄ±ÄŸÄ±nÄ±z yerden devam etmenizi saÄŸlar.

( *Terminalde de bu bilgi yer alÄ±r.* )


### ğŸ¤– (Ä°steÄŸe BaÄŸlÄ±) worker.py DosyasÄ±nda HuggingFace API KullanÄ±mÄ±

`watsonX` yerine HuggingFace API kullanmak da mÃ¼mkÃ¼ndÃ¼r. Ancak  **Ã¼cretsiz sÃ¼rÃ¼mde model seÃ§enekleri ve performans sÄ±nÄ±rlÄ±dÄ±r** .

EylÃ¼l 2023 itibarÄ±yla  **Llama2 Ã¼cretsiz sunulmamakta** , bunun yerine ÅŸu model kullanÄ±lmaktadÄ±r:

```
mistralai/Mistral-7B-Instruct-v0.3
```

---

### âš™ï¸ Gerekli Kurulumlar

AÅŸaÄŸÄ±daki komutlarÄ± terminalinize yapÄ±ÅŸtÄ±rarak gerekli sÃ¼rÃ¼mleri yÃ¼kleyin:

```bash
pip install langchain==0.1.17
pip install huggingface-hub==0.23.4
```

---

### ğŸ” HuggingFace API AnahtarÄ± NasÄ±l AlÄ±nÄ±r?

1. [https://huggingface.co/](https://huggingface.co/) adresine gidin
2. GiriÅŸ yapÄ±n (veya Ã¼cretsiz bir hesap oluÅŸturun)
3. SaÄŸ Ã¼stten **Settings â†’ Access Tokens** bÃ¶lÃ¼mÃ¼ne gidin
4. **New Token** oluÅŸturun
5. "read" veya "write" seÃ§eneÄŸini seÃ§ip token'Ä± kopyalayÄ±n
6. AÅŸaÄŸÄ±daki kodda `"YOUR API KEY"` yerine yapÄ±ÅŸtÄ±rÄ±n

---

### âœ… `init_llm()` Fonksiyonu â€“ HuggingFace ile TamamlanmÄ±ÅŸ Hali

```python
# Dil modelini ve gÃ¶mlemeleri baÅŸlatmak iÃ§in fonksiyon
def init_llm():
    global llm_hub, embeddings

    # HuggingFace API anahtarÄ±nÄ± tanÄ±mla
    os.environ["HUGGINGFACEHUB_API_TOKEN"] = "YOUR API KEY"

    # KullanÄ±lacak modelin repo ID'si
    model_id = "mistralai/Mistral-7B-Instruct-v0.3"

    # Modeli HuggingFaceHub Ã¼zerinden yÃ¼kle
    llm_hub = HuggingFaceHub(
        repo_id=model_id,
        model_kwargs={
            "temperature": 0.1,
            "max_new_tokens": 600,
            "max_length": 600
        }
    )

    # Ã–nceden eÄŸitilmiÅŸ gÃ¶mleme modelini baÅŸlat
    embeddings = HuggingFaceInstructEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={"device": DEVICE}
    )
```

---

Bu yapÄ± sayesinde, HuggingFace Ã¼zerinden Ã§aÄŸrÄ±lan bir LLM modeli ile sohbet botunuz Watsonx yerine tamamen HuggingFace API ile Ã§alÄ±ÅŸacaktÄ±r.


![1748502651429](image/module_5_5_BuildaChatbotforYourData/1748502651429.png)


### ğŸ‡¹ğŸ‡· Kodun TÃ¼rkÃ§e TercÃ¼mesi ve AÃ§Ä±klamalarÄ±yla Tam Hali

```python
import os
import torch
from langchain import PromptTemplate
from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceInstructEmbeddings
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.llms import HuggingFaceHub

# GPU kullanÄ±labilirliÄŸini kontrol et ve uygun cihazÄ± ayarla
DEVICE = "cuda:0" if torch.cuda.is_available() else "cpu"

# Global deÄŸiÅŸkenler
conversation_retrieval_chain = None
chat_history = []
llm_hub = None
embeddings = None

# Dil modelini ve gÃ¶mlemeleri baÅŸlatmak iÃ§in fonksiyon
def init_llm():
    global llm_hub, embeddings

    # HuggingFace API anahtarÄ±nÄ± ayarla
    os.environ["HUGGINGFACEHUB_API_TOKEN"] = "YOUR API KEY"  # â† Buraya kendi anahtarÄ±nÄ±zÄ± ekleyin

    # KullanÄ±lacak modelin repo adÄ±
    model_id = "mistralai/Mistral-7B-Instruct-v0.3"

    # HuggingFaceHub Ã¼zerinden modeli yÃ¼kle
    llm_hub = HuggingFaceHub(
        repo_id=model_id,
        model_kwargs={
            "temperature": 0.1,
            "max_new_tokens": 600,
            "max_length": 600
        }
    )

    # Ã–nceden eÄŸitilmiÅŸ gÃ¶mleme modelini baÅŸlat
    embeddings = HuggingFaceInstructEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={"device": DEVICE}
    )


# PDF belgesini iÅŸlemek iÃ§in fonksiyon
def process_document(document_path):
    global conversation_retrieval_chain

    # PDF belgesini yÃ¼kle
    loader = PyPDFLoader(document_path)
    documents = loader.load()

    # Belgeyi parÃ§alara ayÄ±r (chunk_size=1024, chunk_overlap=64)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=64)
    texts = text_splitter.split_documents(documents)

    # ParÃ§alardan Chroma kullanarak bir vektÃ¶r deposu oluÅŸtur
    db = Chroma.from_documents(texts, embedding=embeddings)

    # Soru-cevap zincirini oluÅŸtur (Retriever + LLM kullanarak)
    conversation_retrieval_chain = RetrievalQA.from_chain_type(
        llm=llm_hub,
        chain_type="stuff",
        retriever=db.as_retriever(search_kwargs={'k': 3}),  # k: LLMâ€™e gÃ¶nderilecek sonuÃ§ sayÄ±sÄ±
        return_source_documents=False,
        input_key="question"
        # chain_type_kwargs={"prompt": prompt}  # EÄŸer Ã¶zel prompt kullanacaksanÄ±z bu satÄ±rÄ± aÃ§Ä±n
    )


# KullanÄ±cÄ± istemini iÅŸlemek iÃ§in fonksiyon
def process_prompt(prompt):
    global conversation_retrieval_chain
    global chat_history

    # Modeli kullanÄ±cÄ± sorusu ve geÃ§miÅŸle sorgula
    output = conversation_retrieval_chain.invoke({
        "question": prompt,
        "chat_history": chat_history
    })

    # Modelin cevabÄ±nÄ± al
    answer = output["result"]

    # Sohbet geÃ§miÅŸine ekle
    chat_history.append((prompt, answer))

    # Cevapta "Helpful Answer:" varsa ayÄ±r
    if "Helpful Answer:" in answer:
        answer = answer.split("Helpful Answer:")[-1].strip()
    else:
        answer = answer.strip()

    # Modelin cevabÄ±nÄ± dÃ¶ndÃ¼r
    return answer


# Dil modelini baÅŸlat
init_llm()
```

---

âœ… **HazÄ±r:** Bu kod ile HuggingFace API ve Ã¶nceden eÄŸitilmiÅŸ gÃ¶mleme modeli kullanarak bir PDF tabanlÄ± sohbet botu oluÅŸturabilir, kullanÄ±cÄ±dan gelen sorulara belge iÃ§eriÄŸine dayalÄ± olarak cevap verebilirsiniz.


### ğŸ¤– Sohbet Botunuzu Ã‡alÄ±ÅŸtÄ±rmak Ä°Ã§in

UygulamanÄ±zÄ± baÅŸlatmak iÃ§in ilk olarak `server.py` dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rmanÄ±z gerekir:

---

### â–¶ï¸ 1. Komutu Terminalde Ã‡alÄ±ÅŸtÄ±rÄ±n

```bash
python3 server.py
```

---

### ğŸŒ 2. UygulamanÄ±zÄ± AÃ§Ä±n

AÅŸaÄŸÄ±daki **butona tÄ±klayÄ±n** veya belirtilen baÄŸlantÄ±yÄ± tarayÄ±cÄ±nÄ±zda aÃ§Ä±n:

> ğŸ’¡ Yeni bir pencere aÃ§Ä±lacak ve sohbet botunuzun arayÃ¼zÃ¼ gÃ¶rÃ¼ntÃ¼lenecektir.

![1748502756093](image/module_5_5_BuildaChatbotforYourData/1748502756093.png)


### ğŸŒ Sunucuyu Anlamak (server.py)

---

### ğŸ§  Sunucu Nedir?

 **Sunucu** , uygulamanÄ±zÄ±n Ã§alÄ±ÅŸtÄ±ÄŸÄ± ve hizmetlerle iletiÅŸim kurduÄŸu bileÅŸendir.

Bu projede **Flask** kullanÄ±lmÄ±ÅŸtÄ±r â€” Python ile web uygulamalarÄ± geliÅŸtirmek iÃ§in kullanÄ±lan  **hafif ve sade bir framework** â€™tÃ¼r.

---

### âš™ï¸ Flask Ne SaÄŸlar?

* Kendi rotalarÄ±nÄ±zÄ± tanÄ±mlayabilirsiniz.
* KullanÄ±cÄ± isteklerini (HTTP request) iÅŸleyebilirsiniz.
* Harici APIâ€™lere veya servislerle baÄŸlantÄ± kurabilirsiniz.
* Ekstra karmaÅŸÄ±k yapÄ± ya da araÃ§lara gerek kalmaz.

---

### ğŸ§© Bu Projede Flask Ne YapÄ±yor?

Bu rehberli projede Flask, **sohbet botunuzun arka ucunu (backend)** yÃ¶netir.

1. KullanÄ±cÄ±, Ã¶n yÃ¼z arayÃ¼zÃ¼nden bir iÅŸlem yapar (soru sorar, dosya yÃ¼kler).
2. Bu istek Flask'a iletilir.
3. Flask bu isteÄŸi iÅŸler, ilgili servise yÃ¶nlendirir ve cevabÄ± geri dÃ¶ndÃ¼rÃ¼r.

---

### ğŸ—‚ï¸ `server.py` DosyasÄ±nÄ±n YapÄ±sÄ±

#### ğŸ“¥ 1. BaÅŸlangÄ±Ã§ta

* `worker` iÃ§e aktarÄ±lÄ±r â†’ `worker.py` dosyasÄ±ndaki mantÄ±ÄŸÄ± kullanÄ±r.
* Flask uygulamasÄ± oluÅŸturulur:
  ```python
  app = Flask(__name__)
  CORS(app)  # CORS politikasÄ± herkese aÃ§Ä±k olacak ÅŸekilde '*' ayarlanÄ±r
  ```

#### ğŸŒ 2. Ä°lk Rota â€“ Ana Sayfa

```python
@app.route('/', methods=['GET'])
def index():
    return render_template('index.html')
```

* KullanÄ±cÄ± uygulamayÄ± aÃ§tÄ±ÄŸÄ±nda `/` yoluna istek gÃ¶nderir.
* Bu `index()` fonksiyonu Ã§alÄ±ÅŸÄ±r ve **Ã¶n yÃ¼z arayÃ¼zÃ¼ olan `index.html`** sayfasÄ±nÄ± yÃ¼kler.

---

### ğŸ“„ 3. DiÄŸer Ä°ki Rota

#### ğŸ“ `process_document_route()`

* KullanÄ±cÄ± **bir PDF dosyasÄ± yÃ¼klediÄŸinde** Ã§aÄŸrÄ±lÄ±r.
* PDF iÅŸlenir, vektÃ¶r veritabanÄ± oluÅŸturulur ve yanÄ±t dÃ¶ner.

#### ğŸ’¬ `process_message_route()`

* KullanÄ±cÄ± **bir soru sorduÄŸunda** Ã§alÄ±ÅŸÄ±r.
* Soru, daha Ã¶nce iÅŸlenmiÅŸ belgeye karÅŸÄ±lÄ±k modele iletilir, cevap oluÅŸturulur ve kullanÄ±cÄ±ya geri gÃ¶nderilir.

---

### ğŸš€ Sunucuyu BaÅŸlatmak

En altta ÅŸu komutla Flask uygulamasÄ± baÅŸlatÄ±lÄ±r:

```python
app.run(host="0.0.0.0", port=8080)
```

* **host="0.0.0.0"** â†’ Her IP'den eriÅŸime izin verir (genelde `localhost` anlamÄ±na gelir).
* **port=8080** â†’ Uygulama bu port Ã¼zerinden Ã§alÄ±ÅŸÄ±r.

---



### ğŸ’» `script.js` DosyasÄ±nÄ± Anlamak (Ä°steÄŸe BaÄŸlÄ±)

Bu JavaScript dosyasÄ±, **kullanÄ±cÄ± arayÃ¼zÃ¼ ve etkileÅŸimleri** yÃ¶netir. `static/` klasÃ¶rÃ¼nde bulunur. Sohbet botu uygulamasÄ±nÄ±n Ã¶n yÃ¼zÃ¼nde gerÃ§ekleÅŸen tÃ¼m dinamik iÅŸlemler buradaki fonksiyonlarla gerÃ§ekleÅŸtirilir.

---

### ğŸ“¨ 1. **Mesaj Ä°ÅŸleme**

KullanÄ±cÄ±nÄ±n mesajÄ±nÄ± sunucuya gÃ¶nderir ve cevabÄ± bekler:

```javascript
const processUserMessage = async (userMessage) => {
  let response = await fetch(baseUrl + "/process-message", {
    method: "POST",
    headers: { Accept: "application/json", "Content-Type": "application/json" },
    body: JSON.stringify({ userMessage: userMessage }),
  });
  response = await response.json();
  console.log(response);
  return response;
};
```

---

### â³ 2. **YÃ¼kleniyor Animasyonu**

Sunucudan yanÄ±t beklenirken "bot yazÄ±yor" animasyonunu gÃ¶sterir/gizler:

```javascript
async function showBotLoadingAnimation() {
  await sleep(200);
  $(".loading-animation")[1].style.display = "inline-block";
  document.getElementById('send-button').disabled = true;
}

function hideBotLoadingAnimation() {
  $(".loading-animation")[1].style.display = "none";
  if(!isFirstMessage){
    document.getElementById('send-button').disabled = false;
  }
}
```

---

### ğŸ’¬ 3. **MesajlarÄ± GÃ¶sterme**

KullanÄ±cÄ± ve bot mesajlarÄ±nÄ± sohbet penceresine ekler:

```javascript
const populateUserMessage = (userMessage, userRecording) => {
  $("#message-input").val("");
  $("#message-list").append(
    `<div class='message-line my-text'><div class='message-box my-text${
      !lightMode ? " dark" : ""
    }'><div class='me'>${userMessage}</div></div></div>`
  );
  scrollToBottom();
};

const populateBotResponse = async (userMessage) => {
  // ... detaylar Ã§Ä±karÄ±ldÄ±
  $("#message-list").append(
    `<div class='message-line'><div class='message-box${!lightMode ? " dark" : ""}'>${response.botResponse.trim()}<br>${uploadButtonHtml}</div></div>`
  );
  scrollToBottom();
};
```

---

### âœ‚ï¸ 4. **KullanÄ±cÄ± Girdisini Temizleme**

Gereksiz boÅŸluklar, satÄ±r atlamalarÄ± ve HTML etiketlerini temizler:

```javascript
const cleanTextInput = (value) => {
  return value
    .trim()
    .replace(/[\n\t]/g, "")
    .replace(/<[^>]*>/g, "")
    .replace(/[<>&;]/g, "");
};
```

---

### ğŸ“ 5. **PDF DosyasÄ± YÃ¼kleme**

KullanÄ±cÄ± PDF yÃ¼klediÄŸinde dosyayÄ± okur ve sunucuya gÃ¶nderir:

```javascript
$("#file-upload").on("change", function () {
  const file = this.files[0];
  const reader = new FileReader();
  reader.onload = async function (e) {
    let response = await fetch(baseUrl + "/process-document", {
      method: "POST",
      headers: { Accept: "application/json", "Content-Type": "application/json" },
      body: JSON.stringify({ fileData: e.target.result }),
    });
    response = await response.json();
  };
  reader.readAsDataURL(file);
});
```

---

### ğŸ”„ 6. **Sohbeti SÄ±fÄ±rlama**

TÃ¼m mesajlarÄ± temizler ve botun ilk yanÄ±tÄ±nÄ± yeniden baÅŸlatÄ±r:

```javascript
$("#reset-button").click(async function () {
  $("#message-list").empty();
  responses.length = 0;
  isFirstMessage = true;
  populateBotResponse();
});
```

---

### ğŸŒ— 7. **AydÄ±nlÄ±k/KaranlÄ±k Mod DeÄŸiÅŸtirme**

KullanÄ±cÄ±nÄ±n arayÃ¼z modunu deÄŸiÅŸtirmesine olanak tanÄ±r:

```javascript
$("#light-dark-mode-switch").change(function () {
  $("body").toggleClass("dark-mode");
  $(".message-box").toggleClass("dark");
  $(".loading-dots").toggleClass("dark");
  $(".dot").toggleClass("dark-dot");
  lightMode = !lightMode;
});
```

---


### ğŸš€ UygulamayÄ± Ã‡alÄ±ÅŸtÄ±rmak ve Docker ile DaÄŸÄ±tmak

---

### â–¶ï¸ 1. Flask UygulamasÄ±nÄ± DoÄŸrudan Ã‡alÄ±ÅŸtÄ±rmak

`server.py` dosyasÄ±nÄ±n en altÄ±nda ÅŸu kod bulunur:

```python
if __name__ == "__main__":
    app.run(debug=True, port=8000, host='0.0.0.0')
```

Bu satÄ±r, dosya doÄŸrudan Ã§alÄ±ÅŸtÄ±rÄ±ldÄ±ÄŸÄ±nda Flask uygulamasÄ±nÄ± baÅŸlatÄ±r.

* `debug=True`: Kod deÄŸiÅŸtiÄŸinde sunucuyu otomatik yeniden baÅŸlatÄ±r.
* `port=8000`: Uygulama bu port Ã¼zerinden Ã§alÄ±ÅŸÄ±r.
* `host='0.0.0.0'`: Her IPâ€™den eriÅŸime izin verir (yerel aÄŸ dahil).

---

### ğŸ³ 2. Docker Container OluÅŸturma

 **Docker** , uygulamayÄ± ve tÃ¼m baÄŸÄ±mlÄ±lÄ±klarÄ±nÄ± birlikte paketleyerek farklÄ± ortamlarda aynÄ± ÅŸekilde Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlar.

**Avantajlar:**

* Tek bir Docker image ile daÄŸÄ±tÄ±m kolaylÄ±ÄŸÄ±
* GeliÅŸtirme, test ve prodÃ¼ksiyon ortamlarÄ±nda tutarlÄ±lÄ±k

---

### ğŸ“¦ 3. Dockerfile ve `requirements.txt`

`git clone` komutuyla indirilen proje klasÃ¶rÃ¼ zaten:

* bir `Dockerfile`
* bir `requirements.txt`

dosyasÄ± iÃ§erir.

**Dockerfile** temel olarak ÅŸunlarÄ± yapar:

1. Python ortamÄ± oluÅŸturur
2. TÃ¼m dosyalarÄ± konteynÄ±ra kopyalar
3. Gerekli paketleri yÃ¼kler
4. UygulamayÄ± `python server.py` ile baÅŸlatÄ±r

---

### â–¶ï¸ 4. Docker ile UygulamayÄ± BaÅŸlatmak

```bash
docker build . -t build_chatbot_for_your_data
docker run -p 8000:8000 build_chatbot_for_your_data
```

* `docker build . -t build_chatbot_for_your_data`: UygulamayÄ± derler ve imaj olarak etiketler
* `docker run -p 8000:8000 ...`: 8000 portundan uygulamayÄ± baÅŸlatÄ±r

ğŸ§  **Not:** Her dosya deÄŸiÅŸikliÄŸinde imajÄ± yeniden oluÅŸturmanÄ±z gerekir.

---

### ğŸŒ 5. TarayÄ±cÄ±da UygulamayÄ± AÃ§mak

> ğŸ“Œ Uygulama,  **minitarayÄ±cÄ±da deÄŸil** , **yeni bir sekmede** aÃ§Ä±lmalÄ±dÄ±r.

ğŸ”’ TarayÄ±cÄ±nÄ±z aÃ§Ä±lÄ±r pencereyi engelleyebilir â€” lÃ¼tfen izin verin.

---

### ğŸ›‘ 6. UygulamayÄ± Durdurmak

Ã‡alÄ±ÅŸan containerâ€™Ä± durdurmak iÃ§in:

```text
Ctrl + C  (Mac'te ^ + C)
```

---

### ğŸ” 7. DeÄŸiÅŸikliklerden Sonra Ne YapmalÄ±?

Dosyalarda deÄŸiÅŸiklik yaptÄ±ysanÄ±z:

1. `docker build` komutunu tekrar Ã§alÄ±ÅŸtÄ±rÄ±n
2. `docker run` ile konteyneri yeniden baÅŸlatÄ±n

---



### ğŸ‰ SonuÃ§

Tebrikler! Bu rehberli projeyi baÅŸarÄ±yla tamamladÄ±nÄ±z! ğŸ“

---

### ğŸš€ Neler Ã–ÄŸrendiniz?

* **Retrieval Augmented Search (RAG)** kavramÄ±nÄ± ve bunu Ã¼retici yapay zekada nasÄ±l kullanacaÄŸÄ±nÄ±zÄ±
* **BÃ¼yÃ¼k Dil Modelleri (LLM)** ile Ã§alÄ±ÅŸma yÃ¶ntemlerini
* **VektÃ¶r veritabanlarÄ±** , **gÃ¶mlemeler (embeddings)** ve **LangChain** entegrasyonunu
* Python, Flask ve JavaScript ile **gerÃ§ek bir uygulama (sohbet botu)** oluÅŸturmayÄ±
* Bu uygulamayÄ± **Docker ve Kubernetes** kullanarak paketlemeyi ve daÄŸÄ±tmayÄ±

---

### ğŸŒ BaÅŸarÄ±nÄ±zÄ± PaylaÅŸÄ±n

LinkedIn, Twitter ve diÄŸer sosyal medya platformlarÄ±nda baÅŸarÄ±nÄ±zÄ± paylaÅŸabilirsiniz.

Proje detay sayfasÄ±nda bu paylaÅŸÄ±m iÃ§in hazÄ±r butonlar bulunuyor.

---

### ğŸ§­ Sonraki AdÄ±mlar

**Ãœretici Yapay Zeka (Generative AI)** ve **LLM'ler** ilginizi Ã§ekiyorsa:

ğŸ§ª [IBM watsonx.ai](https://www.ibm.com/watsonx) iÃ§in Ã¼cretsiz deneme hesabÄ± alÄ±n!

WatsonX platformu 3 ana Ã¼rÃ¼nden oluÅŸur:

* **watsonx.ai studio:** temel modeller, Ã¼retici yapay zeka ve makine Ã¶ÄŸrenimi iÃ§in
* **watsonx.data:** aÃ§Ä±k lakehouse mimarisiyle veri yÃ¶netimi
* **watsonx.governance:** sorumlu, ÅŸeffaf ve aÃ§Ä±klanabilir AI Ã§Ã¶zÃ¼mleri geliÅŸtirmek iÃ§in

---

### ğŸ› ï¸ DerinleÅŸmek Ä°Ã§in Ã–nerilen Projeler

#### ğŸ—£ï¸ Sesli Asistan OluÅŸturun:

**â¡ï¸ Create a voice assistant with IBM Watson**

#### ğŸ§  LangChain ile Uygulama GeliÅŸtirme:

**â¡ï¸ Create AI-powered apps with open source LangChain**

Dosya analizi, grafik Ã¼retme ve daha fazlasÄ±nÄ± entegre etmek iÃ§in LangChainâ€™in olanaklarÄ±nÄ± keÅŸfedin.

---

Tebrikler, Ã¼retici yapay zeka yolculuÄŸunuzda Ã¶nemli bir adÄ±mÄ± baÅŸarÄ±yla tamamladÄ±nÄ±z! ğŸ‘
