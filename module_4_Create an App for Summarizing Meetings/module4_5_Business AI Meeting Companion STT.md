Ä°ÅŸte metnin TÃ¼rkÃ§eye Ã§evrilmiÅŸ ve konu baÅŸlÄ±klarÄ±na gÃ¶re dÃ¼zenlenmiÅŸ hali:

---

### ğŸ¤– GiriÅŸ: Ä°ÅŸ DÃ¼nyasÄ± Ä°Ã§in Yapay Zeka ToplantÄ± AsistanÄ±

Bir iÅŸ toplantÄ±sÄ±na katÄ±ldÄ±ÄŸÄ±nÄ±zÄ± hayal edin; tÃ¼m konuÅŸmalar geliÅŸmiÅŸ bir yapay zeka uygulamasÄ± tarafÄ±ndan kaydediliyor. Bu uygulama yalnÄ±zca konuÅŸmalarÄ± yÃ¼ksek doÄŸrulukla metne dÃ¶nÃ¼ÅŸtÃ¼rmekle kalmÄ±yor, aynÄ± zamanda toplantÄ±nÄ±n Ã¶zetini Ã§Ä±karÄ±yor ve alÄ±nan kararlar ile Ã¶nemli noktalarÄ± vurguluyor.

Bu projede:

* **KonuÅŸmayÄ± metne dÃ¶nÃ¼ÅŸtÃ¼rmek** iÃ§in **OpenAI Whisper** kullanacaÄŸÄ±z.
* ArdÄ±ndan **IBM Watson AI** yardÄ±mÄ±yla metni Ã¶zetleyip kilit noktalarÄ± Ã§Ä±karacaÄŸÄ±z.
* KullanÄ±cÄ± arayÃ¼zÃ¼ iÃ§in ise **Hugging Face Gradio** ile bir uygulama geliÅŸtireceÄŸiz.

---

### ğŸ¯ Ã–ÄŸrenme Hedefleri

Bu uygulamalÄ± laboratuvar Ã§alÄ±ÅŸmasÄ±nÄ± tamamladÄ±ktan sonra ÅŸunlarÄ± yapabileceksiniz:

* Hugging Face Hub Ã¼zerinden bir model kullanarak Python betiÄŸi oluÅŸturmak, modelin Ã§Ä±ktÄ±sÄ±nÄ± etkileyen bazÄ± temel parametreleri tanÄ±mlamak ve farklÄ± LLM modelleri arasÄ±nda nasÄ±l geÃ§iÅŸ yapÄ±lacaÄŸÄ±nÄ± temel dÃ¼zeyde kavramak.
* OpenAIâ€™nin Whisper teknolojisini kullanarak ders kayÄ±tlarÄ±nÄ± doÄŸru bir ÅŸekilde metne dÃ¶nÃ¼ÅŸtÃ¼rmek.
* IBM Watson AI ile transkripte edilmiÅŸ ders iÃ§eriklerini etkili bir ÅŸekilde Ã¶zetlemek ve anahtar noktalarÄ± Ã§Ä±karmak.
* Hugging Face Gradio ile sezgisel ve kullanÄ±cÄ± dostu bir arayÃ¼z oluÅŸturmak, bÃ¶ylece Ã¶ÄŸrenciler ve eÄŸitmenler iÃ§in kullanÄ±m kolaylÄ±ÄŸÄ± saÄŸlamak.

![1748499290519](image/module4_5_BusinessAIMeetingCompanionSTT/1748499290519.png)


---

### ğŸ§ª Ortam HazÄ±rlÄ±ÄŸÄ±

Python sanal ortamÄ± oluÅŸturarak ve gerekli kÃ¼tÃ¼phaneleri kurarak ortama hazÄ±rlÄ±kla baÅŸlayalÄ±m. Terminalde aÅŸaÄŸÄ±daki komutlarÄ± Ã§alÄ±ÅŸtÄ±rÄ±n:

```bash
pip3 install virtualenv 
virtualenv my_env # my_env adÄ±nda sanal ortam oluÅŸtur
source my_env/bin/activate # my_env ortamÄ±nÄ± etkinleÅŸtir
```

---

### ğŸ“¦ Gerekli KÃ¼tÃ¼phanelerin Kurulumu

Sanal ortam etkinleÅŸtirildikten sonra, proje iÃ§in gerekli olan kÃ¼tÃ¼phaneleri yÃ¼kleyin. Bu iÅŸlem biraz zaman alabilir â˜•ï¸â˜•ï¸:

```bash
# my_env ortamÄ±nda gerekli kÃ¼tÃ¼phanelerin kurulumu
pip install transformers==4.36.0 torch==2.1.1 gradio==5.23.2 langchain==0.0.343 ibm_watson_machine_learning==1.0.335 huggingface-hub==0.28.1
```

â³ Bir fincan kahve hazÄ±rlayÄ±n, yÃ¼kleme birkaÃ§ dakika sÃ¼rebilir.

```
      )  (
     (   ) )
      ) ( (
    _______)_
 .-'---------|  
( C|/\/\/\/\/|
 '-./\/\/\/\/|
   '_________'
    '-------'
```

---

### ğŸ§ ffmpeg Kurulumu

Python ile ses dosyalarÄ± Ã¼zerinde Ã§alÄ±ÅŸabilmek iÃ§in `ffmpeg` adlÄ± yardÄ±mcÄ± yazÄ±lÄ±mÄ±n kurulmasÄ± gerekir.

Ä°lk olarak sistem paketlerini gÃ¼ncelleyin:

```bash
sudo apt update
```

ArdÄ±ndan `ffmpeg` paketini yÃ¼kleyin:

```bash
sudo apt install ffmpeg -y
```

---

### ğŸ“„ Ek Bilgi: Whisper LisansÄ±

OpenAI tarafÄ±ndan geliÅŸtirilen **Whisper** projesi GitHub Ã¼zerinden eriÅŸilebilir durumdadÄ±r. Kod ve model aÄŸÄ±rlÄ±klarÄ± **MIT LisansÄ±** altÄ±nda paylaÅŸÄ±lmÄ±ÅŸtÄ±r. AyrÄ±ntÄ±lÄ± bilgi iÃ§in `LICENSE` dosyasÄ±nÄ± inceleyebilirsiniz.


Ä°ÅŸte metnin TÃ¼rkÃ§eye Ã§evrilmiÅŸ ve baÅŸlÄ±klandÄ±rÄ±lmÄ±ÅŸ hali:

---

### ğŸ—£ï¸ AdÄ±m 1: Sesten Metne (Speech-to-Text)

Ä°lk olarak, **OpenAI Whisper** kullanarak basit bir sesten metne dÃ¶nÃ¼ÅŸtÃ¼rme dosyasÄ± oluÅŸturalÄ±m.

ğŸ“ **Ã–rnek ses dosyasÄ±nÄ±** test etmek iÃ§in indirmeniz gerekiyor:

 **BaÄŸlantÄ±** : [Sample voice link](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX04C6EN/Testing%20speech%20to%20text.mp3)

Bir Python dosyasÄ± oluÅŸturun ve adÄ±nÄ± `simple_speech2text.py` koyun. AÅŸaÄŸÄ±daki kodu dosyaya yapÄ±ÅŸtÄ±rarak Ã§alÄ±ÅŸtÄ±rÄ±n:

```python
import requests

# Ä°ndirilecek ses dosyasÄ±nÄ±n URL'si
url = "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX04C6EN/Testing%20speech%20to%20text.mp3"

# URL'ye GET isteÄŸi gÃ¶ndererek dosyayÄ± indir
response = requests.get(url)

# Ses dosyasÄ±nÄ±n yerel olarak kaydedileceÄŸi dosya yolu
audio_file_path = "downloaded_audio.mp3"

# Ä°steÄŸin baÅŸarÄ±lÄ± olup olmadÄ±ÄŸÄ±nÄ± kontrol et (durum kodu 200)
if response.status_code == 200:
    # BaÅŸarÄ±lÄ±ysa iÃ§eriÄŸi belirtilen dosya yoluna yaz
    with open(audio_file_path, "wb") as file:
        file.write(response.content)
    print("Dosya baÅŸarÄ±yla indirildi")
else:
    # Ä°stek baÅŸarÄ±sÄ±zsa hata mesajÄ± yazdÄ±r
    print("Dosya indirilemedi")
```

---

### â–¶ï¸ Python DosyasÄ±nÄ± Ã‡alÄ±ÅŸtÄ±rma

Terminalde aÅŸaÄŸÄ±daki komutu Ã§alÄ±ÅŸtÄ±rarak Python dosyasÄ±nÄ± test edin:

```bash
python3 simple_speech2text.py
```

BaÅŸarÄ±lÄ± Ã§alÄ±ÅŸtÄ±rmadan sonra, dosya yÃ¶neticisinde `downloaded_audio.mp3` adlÄ± ses dosyasÄ±nÄ± gÃ¶rmelisiniz.

![1748499427536](image/module4_5_BusinessAIMeetingCompanionSTT/1748499427536.png)


---

### ğŸ“ OpenAI Whisper ile Sesin Metne DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmesi

Bir Ã¶nceki Python dosyasÄ±nÄ±n iÃ§eriÄŸini aÅŸaÄŸÄ±daki kod ile **deÄŸiÅŸtirin** (`simple_speech2text.py`).

Bu kod, Hugging Face Transformers kÃ¼tÃ¼phanesi Ã¼zerinden OpenAI'nin **Whisper** modelini kullanarak ses dosyasÄ±nÄ± metne dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r:

```python
import torch
from transformers import pipeline

# Hugging Face Transformers Ã¼zerinden sesten metne dÃ¶nÃ¼ÅŸtÃ¼rme pipeline'Ä±nÄ± baÅŸlat
# "openai/whisper-tiny.en" modeli otomatik konuÅŸma tanÄ±ma (ASR) iÃ§in kullanÄ±lÄ±yor
# chunk_length_s parametresi sesin kaÃ§ saniyelik parÃ§alar halinde iÅŸleneceÄŸini belirtir
pipe = pipeline(
  "automatic-speech-recognition",
  model="openai/whisper-tiny.en",
  chunk_length_s=30,
)

# Transkribe edilecek ses dosyasÄ±nÄ±n yolu
sample = 'downloaded_audio.mp3'

# Ses tanÄ±ma iÅŸlemini gerÃ§ekleÅŸtir
# batch_size=8 ile aynÄ± anda kaÃ§ parÃ§anÄ±n iÅŸleneceÄŸi belirtilir
# SonuÃ§ prediction deÄŸiÅŸkeninde saklanÄ±r, transkript "text" anahtarÄ±nda yer alÄ±r
prediction = pipe(sample, batch_size=8)["text"]

# Transkripti konsola yazdÄ±r
print(prediction)
```

---

### â–¶ï¸ Python DosyasÄ±nÄ± Ã‡alÄ±ÅŸtÄ±rÄ±n

AÅŸaÄŸÄ±daki komutu terminalde Ã§alÄ±ÅŸtÄ±rarak dosyayÄ± test edin:

```bash
python3 simple_speech2text.py
```

ğŸ“„ Ekranda ses dosyasÄ±nÄ±n yazÄ±ya dÃ¶kÃ¼lmÃ¼ÅŸ hali gÃ¶rÃ¼ntÃ¼lenecektir.

![1748499484480](image/module4_5_BusinessAIMeetingCompanionSTT/1748499484480.png)


---

### ğŸ–¼ï¸ Sonraki AdÄ±m: Gradio ile Uygulama ArayÃ¼zÃ¼ OluÅŸturma

Bu adÄ±mda, uygulamamÄ±z iÃ§in **kullanÄ±cÄ± dostu bir arayÃ¼z** oluÅŸturmak amacÄ±yla **Gradio** kÃ¼tÃ¼phanesini kullanacaÄŸÄ±z.

Gradio, makine Ã¶ÄŸrenimi modelleriyle etkileÅŸimli web tabanlÄ± arayÃ¼zler oluÅŸturmayÄ± kolaylaÅŸtÄ±rÄ±r. Bu sayede kullanÄ±cÄ±lar, ses dosyasÄ±nÄ± yÃ¼kleyip sonuÃ§larÄ± doÄŸrudan tarayÄ±cÄ±dan gÃ¶rebilirler.

ğŸ“Œ Bir sonraki adÄ±mda, Whisper modelini Gradio arayÃ¼zÃ¼ne entegre eden bir Python uygulamasÄ± oluÅŸturacaÄŸÄ±z. HazÄ±rsanÄ±z Gradio bileÅŸenlerini tanÄ±mlayarak baÅŸlayabiliriz.



### ğŸ’¬ Gradio ArayÃ¼zÃ¼: Basit Bir Demo OluÅŸturma

Bu proje boyunca, Gradio arayÃ¼zÃ¼ kullanarak farklÄ± bÃ¼yÃ¼k dil modeli (LLM) uygulamalarÄ± oluÅŸturacaÄŸÄ±z. Gradioâ€™yu daha yakÄ±ndan tanÄ±mak iÃ§in basit bir uygulama oluÅŸturalÄ±m:

ğŸ“ Proje dizinindeyken bir Python dosyasÄ± oluÅŸturun ve adÄ±nÄ± `hello.py` olarak belirleyin.

`hello.py` dosyasÄ±nÄ± aÃ§Ä±n, aÅŸaÄŸÄ±daki Python kodunu yapÄ±ÅŸtÄ±rÄ±n ve dosyayÄ± kaydedin:

```python
import gradio as gr

def greet(name):
    return "Hello " + name + "!"

demo = gr.Interface(fn=greet, inputs="text", outputs="text")
demo.launch(server_name="0.0.0.0", server_port=7860)
```

---

### ğŸ§© Kod AÃ§Ä±klamasÄ±

YukarÄ±daki kod, `demo` adÄ±nda bir `gr.Interface` nesnesi oluÅŸturur. Bu arayÃ¼z, `greet` fonksiyonunu basit bir metin-girdi / metin-Ã§Ä±ktÄ± yapÄ±sÄ± iÃ§inde sararak kullanÄ±cÄ±yla etkileÅŸimli hale getirir.

`gr.Interface` sÄ±nÄ±fÄ± Ã¼Ã§ temel parametreyle baÅŸlatÄ±lÄ±r:

* `fn`: ArayÃ¼zÃ¼n etrafÄ±na sarÄ±lacaÄŸÄ± fonksiyon
* `inputs`: Girdi bileÅŸeni (Ã¶rneÄŸin â€œtextâ€, â€œimageâ€ veya â€œaudioâ€)
* `outputs`: Ã‡Ä±ktÄ± bileÅŸeni (Ã¶rneÄŸin â€œtextâ€, â€œimageâ€ veya â€œlabelâ€)

`demo.launch()` satÄ±rÄ±, bu uygulamayÄ± yerel bir sunucuda baÅŸlatarak eriÅŸilebilir hale getirir.

---

### ğŸš€ Demo UygulamayÄ± BaÅŸlatma

Terminale geri dÃ¶nÃ¼n ve satÄ±r baÅŸÄ±nda `my_env` sanal ortam adÄ±nÄ±n gÃ¶rÃ¼ndÃ¼ÄŸÃ¼nden emin olun.

AÅŸaÄŸÄ±daki komutu Ã§alÄ±ÅŸtÄ±rarak Python betiÄŸini Ã§alÄ±ÅŸtÄ±rÄ±n:

```bash
python3 hello.py
```

Kod Ã§alÄ±ÅŸtÄ±ÄŸÄ±nda, uygulama bir yerel sunucu aracÄ±lÄ±ÄŸÄ±yla servis edilir. TarayÄ±cÄ±nÄ±zda verilen baÄŸlantÄ±ya tÄ±klayarak bu basit uygulamayÄ± gÃ¶rÃ¼ntÃ¼leyebilirsiniz.

âœï¸ Girdi kÄ±smÄ±na Ã¶rneÄŸin `Bob` yazarsanÄ±z, Ã§Ä±ktÄ± olarak `Hello Bob!` mesajÄ± alÄ±rsÄ±nÄ±z.

Uygulamayla oynamaktan Ã§ekinmeyin!

![1748499644862](image/module4_5_BusinessAIMeetingCompanionSTT/1748499644862.png)


### â Uygulamadan Ã‡Ä±kÄ±ÅŸ

Uygulamayla iÅŸiniz bittiyse ve Ã§Ä±kmak istiyorsanÄ±z, terminalde **Ctrl + C** tuÅŸlarÄ±na basarak uygulamayÄ± durdurabilir, ardÄ±ndan tarayÄ±cÄ±daki uygulama sekmesini kapatabilirsiniz.

---

### ğŸ¨ Daha Fazla Ã–zelleÅŸtirme Ã–ÄŸrenmek Ä°steyenler Ä°Ã§in

Gradio hakkÄ±nda daha fazla Ã¶zelleÅŸtirme Ã¶ÄŸrenmek isterseniz, **â€œBring your Machine Learning model to life with Gradioâ€** adlÄ± rehberli projeyi inceleyebilirsiniz. Bu projeyi **cognitiveclass.ai** platformunda **Courses & Projects** bÃ¶lÃ¼mÃ¼nde bulabilirsiniz.

---

### ğŸš§ Devam Eden Projede Gradio KullanÄ±mÄ±

Bu projenin geri kalanÄ±nda, bÃ¼yÃ¼k dil modeli (LLM) tabanlÄ± uygulamalar iÃ§in arayÃ¼z olarak  **Gradio** 'yu kullanmaya devam edeceÄŸiz.


### ğŸ§¾ AdÄ±m 2: Ses Transkripsiyon UygulamasÄ± OluÅŸturma

Bu adÄ±mda, bir Python dosyasÄ± oluÅŸturup adÄ±nÄ± `speech2text_app.py` koyun.

ArdÄ±ndan, aÅŸaÄŸÄ±daki kodu kullanarak eksik yerleri Ã¶nceki adÄ±mdan (Step 1) elde ettiÄŸimiz bilgilerle  **tamamlayÄ±n** :

```python
import torch
from transformers import pipeline
import gradio as gr

# OpenAI Whisper modeli ile sesi metne dÃ¶nÃ¼ÅŸtÃ¼ren fonksiyon
def transcript_audio(audio_file):
    # KonuÅŸma tanÄ±ma pipeline'Ä±nÄ± baÅŸlat
    pipe = pipeline(
        "automatic-speech-recognition",
        model="openai/whisper-tiny.en",
        chunk_length_s=30,
    )
  
    # Ses dosyasÄ±nÄ± transkribe et ve sonucu dÃ¶ndÃ¼r
    result = pipe(audio_file, batch_size=8)["text"]
    return result

# Gradio arayÃ¼z bileÅŸenlerini tanÄ±mla
audio_input = gr.Audio(sources="upload", type="filepath")  # Ses giriÅŸi
output_text = gr.Textbox()  # Metin Ã§Ä±ktÄ±sÄ±

# Gradio arayÃ¼zÃ¼nÃ¼ oluÅŸtur
iface = gr.Interface(
    fn=transcript_audio, 
    inputs=audio_input,
    outputs=output_text, 
    title="Audio Transcription App",
    description="Upload the audio file"
)

# Gradio uygulamasÄ±nÄ± baÅŸlat
iface.launch(server_name="0.0.0.0", server_port=7860)
```

---

### â–¶ï¸ UygulamayÄ± Ã‡alÄ±ÅŸtÄ±rma

Terminalde aÅŸaÄŸÄ±daki komutu Ã§alÄ±ÅŸtÄ±rarak uygulamayÄ± baÅŸlatÄ±n:

```bash
python3 speech2text_app.py
```

---

### ğŸ“¤ Ses DosyasÄ± YÃ¼kleme

* SaÄŸ tÄ±klayarak saÄŸlanan Ã¶rnek ses dosyasÄ±nÄ± indirebilir ve uygulamaya yÃ¼kleyebilirsiniz.
* Alternatif olarak, bilgisayarÄ±nÄ±zdan herhangi bir **MP3 ses dosyasÄ±nÄ±** da yÃ¼kleyebilirsiniz.

---

### ğŸ“„ SonuÃ§

YÃ¼klediÄŸiniz ses dosyasÄ±nÄ±n iÃ§eriÄŸi, uygulama tarafÄ±ndan **metne dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lerek** ekranda gÃ¶rÃ¼ntÃ¼lenecektir.

![1748499713064](image/module4_5_BusinessAIMeetingCompanionSTT/1748499713064.png)


### â¹ï¸ UygulamayÄ± Durdurma

Uygulama Ã§alÄ±ÅŸÄ±rken durdurmak isterseniz, terminalde **Ctrl + C** tuÅŸlarÄ±na basarak uygulamayÄ± sonlandÄ±rabilirsiniz.


### ğŸ¤– AdÄ±m 3: LLM Entegrasyonu â€“ WatsonX Ãœzerinde Llama 3 KullanÄ±mÄ±

#### Basit Bir LLM UygulamasÄ± Ã‡alÄ±ÅŸtÄ±rma

Bu adÄ±mda, **LLM (Large Language Model)** ile metin Ã¼retmeyi deneyeceÄŸiz. Bunun iÃ§in bir Python dosyasÄ± oluÅŸturun ve adÄ±nÄ± `simple_llm.py` koyun.

EÄŸer **Llama 3** modelini IBM WatsonX Ã¼zerinde kullanmak istiyorsanÄ±z, aÅŸaÄŸÄ±daki yÃ¶nergeleri takip edebilirsiniz:

---

### âš™ï¸ Kodun Ä°ÅŸleyiÅŸi

1. **Kimlik Bilgilerini Ayarlama**

   Skills Network ekibi tarafÄ±ndan Ã¶nceden tanÄ±mlandÄ±ÄŸÄ± iÃ§in IBM servislerine eriÅŸim iÃ§in gereken kimlik bilgileri sizin yerinize zaten tanÄ±mlanmÄ±ÅŸtÄ±r.
2. **Parametreleri Belirtme**

   * `MAX_NEW_TOKENS`: Modelin bir seferde Ã¼retebileceÄŸi maksimum token (kelime) sayÄ±sÄ±nÄ± belirler.
   * `TEMPERATURE`: Ãœretilen metnin yaratÄ±cÄ± mÄ± yoksa tahmin edilebilir mi olacaÄŸÄ±nÄ± belirleyen sÄ±caklÄ±k parametresi.
3. **Llama 3 Modelini BaÅŸlatma**

   Belirtilen model ID, kimlik bilgileri, parametreler ve proje ID kullanÄ±larak `LLAMA3_model` nesnesi oluÅŸturulur.
4. **Model Nesnesi OluÅŸturma ve Sorgulama**

   `llm` adlÄ± nesne ile `WatsonxLLM` sÄ±nÄ±fÄ± Ã¼zerinden modelle etkileÅŸim kurulur. Soru olarak `"How to read a book effectively?"` girilir ve Ã¼retilen yanÄ±t yazdÄ±rÄ±lÄ±r.

---

### ğŸ’» Python Kod DosyasÄ± (`simple_llm.py`)

```python
from ibm_watson_machine_learning.foundation_models import Model
from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM
from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams

# IBM Cloud eriÅŸim bilgileri
my_credentials = {
    "url": "https://us-south.ml.cloud.ibm.com"
}

# Model iÃ§in parametreler
params = {
    GenParams.MAX_NEW_TOKENS: 700,  # Ãœretilecek maksimum token sayÄ±sÄ±
    GenParams.TEMPERATURE: 0.1      # DÃ¼ÅŸÃ¼k sÄ±caklÄ±k = daha kararlÄ± cevaplar
}

# Llama 3 modelini baÅŸlat
LLAMA2_model = Model(
    model_id='meta-llama/llama-3-2-11b-vision-instruct',
    credentials=my_credentials,
    params=params,
    project_id="skills-network"
)

# WatsonxLLM nesnesiyle model arayÃ¼zÃ¼ oluÅŸtur
llm = WatsonxLLM(LLAMA2_model)

# Modelden yanÄ±t al ve yazdÄ±r
print(llm("How to read a book effectively?"))
```

---

### â–¶ï¸ Scripti Ã‡alÄ±ÅŸtÄ±rma

Terminalde aÅŸaÄŸÄ±daki komutla Python dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rÄ±n:

```bash
python3 simple_llm.py
```

---

### ğŸ“„ Beklenen Ã‡Ä±ktÄ±

Komut Ã§alÄ±ÅŸtÄ±ktan sonra, modelin `"How to read a book effectively?"` sorusuna verdiÄŸi yanÄ±t terminal ekranÄ±nÄ±zda gÃ¶rÃ¼necektir. Bu, Llama 3 modelinin metin Ã¼retme yeteneÄŸini doÄŸrudan test etmenizi saÄŸlar.

![1748499811002](image/module4_5_BusinessAIMeetingCompanionSTT/1748499811002.png)


### ğŸ§  watsonx Llama 2'nin GÃ¼Ã§lÃ¼ YanÄ±t PerformansÄ±

Watsonx Ã¼zerinde Ã§alÄ±ÅŸan **Llama 2** modelinin nasÄ±l kaliteli bir yanÄ±t sunduÄŸunu gÃ¶rebilirsiniz.


### ğŸ§© AdÄ±m 4: Her Åeyi BirleÅŸtirme

Yeni bir Python dosyasÄ± oluÅŸturun ve adÄ±nÄ± **speech_analyzer.py** koyun.

Bu uygulamada, bir dil modeli (LLM) Ã¶rneÄŸi kuracaÄŸÄ±z; bu IBM WatsonxLLM, HuggingFaceHub veya OpenAI modeli olabilir. Daha sonra bir prompt (istem) ÅŸablonu oluÅŸturacaÄŸÄ±z. Bu ÅŸablonlar, dil modelleri iÃ§in istem Ã¼retmekte yapÄ±landÄ±rÄ±lmÄ±ÅŸ rehberlerdir ve Ã§Ä±ktÄ± dÃ¼zenine yardÄ±mcÄ± olur (daha fazla bilgi iÃ§in langchain prompt template bÃ¶lÃ¼mÃ¼ne bakÄ±nÄ±z).

ArdÄ±ndan, OpenAI Whisper modelini kullanan bir transkripsiyon fonksiyonu geliÅŸtireceÄŸiz. Bu fonksiyon, Gradio uygulama arayÃ¼zÃ¼ aracÄ±lÄ±ÄŸÄ±yla yÃ¼klenen bir ses dosyasÄ±nÄ± (tercihen .mp3 formatÄ±nda) alÄ±r ve konuÅŸmayÄ± metne dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r. Transkripte edilen metin daha sonra bir LLMChain'e gÃ¶nderilir; bu yapÄ±, metni prompt ÅŸablonuyla birleÅŸtirir ve seÃ§ilen LLM'e iletir. Son olarak, LLM'den alÄ±nan Ã§Ä±ktÄ± Gradio uygulamasÄ±nÄ±n Ã§Ä±ktÄ± metin kutusunda gÃ¶rÃ¼ntÃ¼lenir.

Ã‡Ä±ktÄ± aÅŸaÄŸÄ±daki gibi gÃ¶rÃ¼nmelidir:

![1748499942081](image/module4_5_BusinessAIMeetingCompanionSTT/1748499942081.png)


### ğŸ’¡ AlÄ±ÅŸtÄ±rma: Eksik KodlarÄ± Tamamlama

AÅŸaÄŸÄ±da verilen Python kodunda eksik yerler doldurulmuÅŸtur:

```python
import torch
import os
import gradio as gr
# from langchain.llms import OpenAI
from langchain.llms import HuggingFaceHub
from transformers import pipeline
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM
from ibm_watson_machine_learning.foundation_models.utils.enums import DecodingMethods
from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams
from ibm_watson_machine_learning.foundation_models import Model

#######------------- LLM-------------####
# IBM WatsonxLLM Ã¶rneÄŸi baÅŸlatÄ±lÄ±yor
my_credentials = {
    "url": "https://us-south.ml.cloud.ibm.com"
}
params = {
    GenParams.MAX_NEW_TOKENS: 700,
    GenParams.TEMPERATURE: 0.1
}
llama_model = Model(
    model_id='meta-llama/llama-3-2-11b-vision-instruct',
    credentials=my_credentials,
    params=params,
    project_id="skills-network"
)
llm = WatsonxLLM(llama_model)

#######------------- Prompt Template-------------####
# LLAMA2 yapÄ±sÄ±na gÃ¶re biÃ§imlendirilmiÅŸ istem ÅŸablonu
temp = """
<s><<SYS>>
List the key points with details from the context: 
[INST] The context : {context} [/INST] 
<</SYS>>
"""

pt = PromptTemplate(
    input_variables=["context"],
    template=temp
)

prompt_to_LLAMA2 = LLMChain(llm=llm, prompt=pt)

#######------------- Speech2text-------------####
def transcript_audio(audio_file):
    # KonuÅŸma tanÄ±ma pipeline'Ä±nÄ± baÅŸlat
    pipe = pipeline(
        "automatic-speech-recognition",
        model="openai/whisper-tiny.en",
        chunk_length_s=30
    )
    # Ses dosyasÄ±nÄ± transkribe et
    transcript_txt = pipe(audio_file, batch_size=8)["text"]
    # Metni isteme yerleÅŸtirip LLM'e gÃ¶nder
    result = prompt_to_LLAMA2.run(transcript_txt)
    return result

#######------------- Gradio-------------####
audio_input = gr.Audio(sources="upload", type="filepath")
output_text = gr.Textbox()

iface = gr.Interface(
    fn=transcript_audio,
    inputs=audio_input,
    outputs=output_text,
    title="Speech Analyzer",
    description="Upload an MP3 file. The app will transcribe and extract key points."
)

iface.launch(server_name="0.0.0.0", server_port=7860)
```

---

### â–¶ï¸ Ã‡alÄ±ÅŸtÄ±rma

Terminalde:

```bash
python3 speech_analyzer.py
```

ğŸŸ¢ EÄŸer hata yoksa uygulamanÄ±z yerel sunucuda baÅŸlar ve tarayÄ±cÄ±dan ses yÃ¼kleyerek test edebilirsiniz.


### ğŸ SonuÃ§

ğŸ‰ **Tebrikler!** Bu projeyi baÅŸarÄ±yla tamamladÄ±nÄ±z! ArtÄ±k gÃ¼Ã§lÃ¼ **BÃ¼yÃ¼k Dil Modelleri (LLM)** ile konuÅŸmadan metne dÃ¶nÃ¼ÅŸtÃ¼rme gÃ¶revleri iÃ§in saÄŸlam bir temel oluÅŸturmuÅŸ oldunuz.

Ä°ÅŸte baÅŸardÄ±klarÄ±nÄ±zÄ±n kÄ±sa bir Ã¶zeti:

---

### ğŸ§  LLM ile Metin Ãœretimi

Hugging Face Hub'dan bir model kullanarak metin Ã¼reten bir Python betiÄŸi oluÅŸturdunuz.

Model Ã§Ä±ktÄ±sÄ±nÄ± etkileyen temel parametreleri Ã¶ÄŸrendiniz ve farklÄ± LLM modelleri arasÄ±nda geÃ§iÅŸ yapmanÄ±n temellerini kavradÄ±nÄ±z.

---

### ğŸ—£ï¸ KonuÅŸmadan Metne DÃ¶nÃ¼ÅŸtÃ¼rme

OpenAI'nin **Whisper** teknolojisini kullanarak, sesli ders kayÄ±tlarÄ±nÄ± doÄŸru bir ÅŸekilde metne dÃ¶nÃ¼ÅŸtÃ¼rdÃ¼nÃ¼z.

---

### ğŸ“ Ä°Ã§erik Ã–zetleme

IBM Watson AI'yi kullanarak transkripte edilmiÅŸ konuÅŸmalarÄ± etkili ÅŸekilde Ã¶zetlediniz ve anahtar noktalarÄ± Ã§Ä±kardÄ±nÄ±z.

---

### ğŸ’» KullanÄ±cÄ± ArayÃ¼zÃ¼ GeliÅŸtirme

**Hugging Face Gradio** kullanarak sezgisel ve kullanÄ±cÄ± dostu bir arayÃ¼z oluÅŸturdunuz.

Bu sayede Ã¶ÄŸrenciler ve eÄŸitmenler iÃ§in kolay kullanÄ±labilir bir uygulama geliÅŸtirdiniz.

---
